{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential, Model\n",
    "    from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
    "    from tensorflow.keras.optimizers import SGD\n",
    "except:\n",
    "    os.system('python3 -m pip install tensorflow==2.7')\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential, Model\n",
    "    from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
    "    from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks applied in Football Analytics I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During my project, I built different kinds of Artificial Neural Network (ANN) models in order to solve some problems and see new applications of data in sports analytics. To do so, I used TensorFlow, an open-source library for machine learning that is very optimised for this kind of models. I chose this library as it is one of the most popular ones.\n",
    "\n",
    "As I started working with TensorFlow, I encountered a few backward-compatibility issues due to newer versions of the library not being able to run code written for older versions. Therefore, I made sure to use the same version of TensorFlow as the one used to prepare this project: 2.7 (latest stable version at the moment of writing this).\n",
    "\n",
    "During the project, I built 3 different kinds of ANN, each to solve a different problem using a different dataset. I started by designing and training a Multilayer Perceptron (MLP) (or feedforward ANN) to compute a metric called Post-Shot Expected Goals (PSxG), which is the part discussed in this Notebook. Later on, I constructed a Convolutional Neural Network (CNN) to be used as an action selection model through spatial data. Finally, I designed an Autoencoder to reduce the dimensionality of a dataset consisting of aggregated player stats in order to apply clustering algorithms and find different groups of players.\n",
    "\n",
    "It is worth noting that ANNs allow for lots of hyper-parameters to be tuned, as well as the many possible structures we can use. Finding the most appropriate design is sometimes regarded more as an art rather than a science, given that trying all possible combinations may be too power and time consuming. Due to the time limitation of my project, I only tried to find performance improvements by applying some tweaks on the model for the first exercise. For the rest, I focused on how can the different network structures be helpful given their nature. I didn't use methods such as cross-validation for model selection due to time limitations, but I am aware that these procedures should be used when dealing with real-world problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multilayer Perceptron to create a PSxG model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, I started building a relatively simple neural network to model a metric called PSxG during my project. Similarly to xG, this one also predicts how likely a shot is to become a goal. However, it does so only for shots on target, and it takes into account post-shot information such as the location on the target where the ball is directed to or the speed of the shot. That is why this metric can be used to assess goalkeeping performance.\n",
    "\n",
    "To train the model, I used a dataset that contained information on shots produced during different seasons. As a first step, I performed feature extraction to produce variables capable of expressing the information contained in the dataset in a way that is easier to understand for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_pos_x</th>\n",
       "      <th>origin_pos_y</th>\n",
       "      <th>destination_pos_x</th>\n",
       "      <th>destination_pos_y</th>\n",
       "      <th>destination_pos_z</th>\n",
       "      <th>gk_pos_x</th>\n",
       "      <th>gk_pos_y</th>\n",
       "      <th>duration</th>\n",
       "      <th>is_goal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.890833</td>\n",
       "      <td>0.40125</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.49875</td>\n",
       "      <td>0.45720</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.47375</td>\n",
       "      <td>0.637257</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.815833</td>\n",
       "      <td>0.32375</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>0.49250</td>\n",
       "      <td>0.54864</td>\n",
       "      <td>0.969167</td>\n",
       "      <td>0.47875</td>\n",
       "      <td>0.983675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.53500</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>0.51250</td>\n",
       "      <td>1.18872</td>\n",
       "      <td>0.988333</td>\n",
       "      <td>0.49750</td>\n",
       "      <td>1.384115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.65500</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.47625</td>\n",
       "      <td>0.36576</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.60875</td>\n",
       "      <td>0.962551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.54000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.46000</td>\n",
       "      <td>0.18288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.51250</td>\n",
       "      <td>0.904610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin_pos_x  origin_pos_y  destination_pos_x  destination_pos_y  \\\n",
       "0      0.890833       0.40125             1.0000            0.49875   \n",
       "1      0.815833       0.32375             0.9650            0.49250   \n",
       "2      0.673333       0.53500             0.9875            0.51250   \n",
       "3      0.913333       0.65500             1.0000            0.47625   \n",
       "4      0.916667       0.54000             1.0000            0.46000   \n",
       "\n",
       "   destination_pos_z  gk_pos_x  gk_pos_y  duration  is_goal  \n",
       "0            0.45720  0.965000   0.47375  0.637257        1  \n",
       "1            0.54864  0.969167   0.47875  0.983675        0  \n",
       "2            1.18872  0.988333   0.49750  1.384115        0  \n",
       "3            0.36576  0.945000   0.60875  0.962551        1  \n",
       "4            0.18288  1.000000   0.51250  0.904610        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shots = pd.read_csv(\"data/df_shots.csv\")\n",
    "df_shots.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started by transforming the y-coordinate positions into mirrored coordinates. This transformation consists of converting the y-coordinate values so that they disregard which half of the pitch they are in, which will be helpful in simplifying the model. This assumption is based on the idea that the model will produce symmetric results regarding the center of the field. After transformation, the new variables' value ranges between 0 and 0.5. For example, this transformation should convert two different y-coordinate values such as 0.4 and 0.6 into the same transformed value since they are both at the same distance from the center of the field, the 0.5 position.\n",
    "\n",
    "I applied this transformation to the following variables: origin_pos_y, destination_pos_y, and gk_pos_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_dim = [105,68]\n",
    "\n",
    "def distance_between_two_points(p1, p2):\n",
    "    return np.linalg.norm(p2 - p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_shots['origin_y_dist_center'] = df_shots.apply(lambda row: distance_between_two_points(\n",
    "                                        np.array([row.origin_pos_x,row.origin_pos_y])*pitch_dim,\n",
    "                                        np.array([row.origin_pos_x,0.5])*pitch_dim),axis=1)\n",
    "\n",
    "df_shots['destination_y_dist_center'] = df_shots.apply(lambda row: distance_between_two_points(\n",
    "                                        np.array([row.destination_pos_x,row.destination_pos_y])*pitch_dim,\n",
    "                                        np.array([row.destination_pos_x,0.5])*pitch_dim),axis=1)\n",
    "\n",
    "df_shots['gk_y_dist_center'] = df_shots.apply(lambda row: distance_between_two_points(\n",
    "                                        np.array([row.gk_pos_x,row.gk_pos_y])*pitch_dim,\n",
    "                                        np.array([row.gk_pos_x,0.5])*pitch_dim),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I converted the x-y-coordinates into distances. Before doing so, I first transformed the positions into meters, since the field is 105m long and 68m wide, but both original coordinates were normalized using a range between 0 and 1. The distances that I calculated were the shot distance (origin-destination), the distance between the keeper and the destination (gk-destination), and the distance between the keeper and the shooter (origin-gk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_shots['shot_distance'] = df_shots.apply(lambda row: distance_between_two_points(\n",
    "                                        np.array([row.origin_pos_x,row.origin_pos_y])*pitch_dim,\n",
    "                                        np.array([row.destination_pos_x,row.destination_pos_y])*pitch_dim),axis=1)\n",
    "\n",
    "df_shots['gk_distance_destination'] = df_shots.apply(lambda row: distance_between_two_points(\n",
    "                                        np.array([row.gk_pos_x,row.gk_pos_y])*pitch_dim,\n",
    "                                        np.array([row.destination_pos_x,row.destination_pos_y])*pitch_dim),axis=1)\n",
    "\n",
    "df_shots['gk_distance_shooter'] = df_shots.apply(lambda row: distance_between_two_points(\n",
    "                                        np.array([row.origin_pos_x,row.origin_pos_y])*pitch_dim,\n",
    "                                        np.array([row.gk_pos_x,row.gk_pos_y])*pitch_dim),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I computed 2 different angle variables. Firstly, I calculated the angle between the shot position and the destination of the shot. Secondly, I calculated the angle between the shot position and the goalkeeper. Note that the angle between 2 points can be computed as follows:\n",
    "```python\n",
    "abs(np.arctan2(distance_in_y, distance_in_x))\n",
    "```\n",
    "In this formula, the distances refer to the distance between the 1st and 2nd point for each dimension. To compute an accurate angle, both distances should be expressed in meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def angle_two_points(origin, destination):\n",
    "\n",
    "    distance_in_y = destination[1]-origin[1]\n",
    "    distance_in_x = destination[0]-origin[0]\n",
    "    \n",
    "    return abs(np.arctan2(distance_in_y, distance_in_x))\n",
    "\n",
    "df_shots['angle_origin_destination'] = df_shots.apply(lambda row: angle_two_points(\n",
    "                                            np.array([row.origin_pos_x,row.origin_pos_y])*pitch_dim,\n",
    "                                            np.array([row.destination_pos_x,row.destination_pos_y])*pitch_dim),axis=1)\n",
    "df_shots['angle_origin_gk'] = df_shots.apply(lambda row: angle_two_points(\n",
    "                                            np.array([row.origin_pos_x,row.origin_pos_y])*pitch_dim,\n",
    "                                            np.array([row.gk_pos_x,row.gk_pos_y])*pitch_dim),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see later on, I needed a second response variable since I was trying to solve a classification problem (goal or not goal). To do this, I used one-hot encoding to convert the is_goal variable. Therefore, I ended up having 2 different variables: is_goal and is_not_goal. For goals, is_goal was set to 1 and is_not_goal to 0, and vice versa for not goals. To create the is_not_goal variable, I simply subtracted the is_goal variable from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_shots['is_not_goal'] = abs(df_shots['is_goal']-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the dataframe columns into two new dataframes: X and y. X includes the newly created variables (except for is_not_goal), as well as destination_pos_z and duration. On the other hand, y includes the response variables is_goal and is_not_goal. This split allows for easier handling and manipulation of the data during the modelling stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "explanatory_vars = ['gk_y_dist_center','origin_y_dist_center','destination_y_dist_center',\n",
    "                    'destination_pos_z','duration','shot_distance','gk_distance_destination','gk_distance_shooter',\n",
    "                    'angle_origin_destination','angle_origin_gk']\n",
    "response_vars = ['is_goal','is_not_goal']\n",
    "\n",
    "X = df_shots[explanatory_vars]\n",
    "y = df_shots[response_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now split the dataset into train and test, in order to have a set of data different from the one we used to fit the model to obtain performance results on unseen data. Note that, once again, the response variables are unbalanced. Therefore, I will need to use a stratified split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indexes = list(StratifiedShuffleSplit(n_splits=1, test_size=0.33, random_state=1234).split(X, y))[0]\n",
    "train_index, test_index = indexes\n",
    "X_train, X_test, y_train, y_test = X.loc[train_index], X.loc[test_index], y.loc[train_index], y.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(is_goal        0.322249\n",
       " is_not_goal    0.677751\n",
       " dtype: float64,\n",
       " is_goal        0.322272\n",
       " is_not_goal    0.677728\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell presents a group of methods that I will use to train and evaluate the NN models I create. This will be helpful to allow changing model parameters and configurations in a simplified manner. The methods are pretty straight-forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(m, _data, loss=\"mean_squared_error\", metrics=[\"accuracy\", \"mean_squared_error\"], learning_rate=0.1, momentum=0.0, epochs=50, batch_size=64, verbose=1):\n",
    "    evaluate_model(train_model(m, _data, loss, metrics, learning_rate, momentum, epochs, batch_size, verbose), _data, metrics)\n",
    "\n",
    "def train_model(m, _data, loss=\"mean_squared_error\", metrics=[\"accuracy\", \"mean_squared_error\"], learning_rate=0.1, momentum=0.0, epochs=50, batch_size=64, verbose=1):\n",
    "    tf.random.set_seed(1234)\n",
    "    m.compile(optimizer=SGD(learning_rate=learning_rate, momentum=momentum), loss=loss, metrics=metrics)\n",
    "    m.fit(_data[\"X_train\"], _data[\"y_train\"], epochs=epochs , batch_size=batch_size, validation_split=0.2, verbose=verbose)\n",
    "    return m\n",
    "\n",
    "def evaluate_model(m, _data, metrics):\n",
    "    r = m.evaluate(_data[\"X_test\"], _data[\"y_test\"], verbose=0)\n",
    "    print(\"\\n\" + \" | \".join(\"{0}: {1}\".format(metric, value) for metric, value in zip(metrics, r[1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have noticed that apart from the model m, the methods require a dictionary containing the different partitions. Therefore, I will generate this dictionary containing the partitions I have previously generated. I it important to scale the X_* partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(np.array(X_train))\n",
    "X_test_scaled = scaler.fit_transform(np.array(X_test))\n",
    "\n",
    "data = {}\n",
    "data['X_train'] = X_train_scaled\n",
    "data['X_test'] = X_test_scaled\n",
    "data['y_train'] = y_train\n",
    "data['y_test'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to start producing models. And I will begin with the simplest one possible. I will use the default parameters and will construct a neural network without hidden layers. So the only layers contained will be the input layer (with the same dimensions as the input -> 10) and the output layer (with the same dimensions as the output -> 2). The following diagram illustrates the structure mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" style=\"cursor: move;\" width=\"1872\" height=\"939\"><g transform=\"translate(-453.4407967506179,-189.33900437484783) scale(1.3717332885905962)\"><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,279.5, 1009.3333333333334,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,319.5, 1009.3333333333334,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,359.5, 1009.3333333333334,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,399.5, 1009.3333333333334,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,439.5, 1009.3333333333334,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,479.5, 1009.3333333333334,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,519.5, 1009.3333333333334,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,559.5, 1009.3333333333334,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,599.5, 1009.3333333333334,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,639.5, 1009.3333333333334,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,279.5, 1009.3333333333334,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,319.5, 1009.3333333333334,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,359.5, 1009.3333333333334,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,399.5, 1009.3333333333334,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,439.5, 1009.3333333333334,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,479.5, 1009.3333333333334,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,519.5, 1009.3333333333334,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,559.5, 1009.3333333333334,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,599.5, 1009.3333333333334,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M829.3333333333334,639.5, 1009.3333333333334,479.5\"/><circle r=\"10\" class=\"node\" id=\"0_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"829.3333333333334\" cy=\"279.5\"/><circle r=\"10\" class=\"node\" id=\"0_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"829.3333333333334\" cy=\"319.5\"/><circle r=\"10\" class=\"node\" id=\"0_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"829.3333333333334\" cy=\"359.5\"/><circle r=\"10\" class=\"node\" id=\"0_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"829.3333333333334\" cy=\"399.5\"/><circle r=\"10\" class=\"node\" id=\"0_4\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"829.3333333333334\" cy=\"439.5\"/><circle r=\"10\" class=\"node\" id=\"0_5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"829.3333333333334\" cy=\"479.5\"/><circle r=\"10\" class=\"node\" id=\"0_6\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"829.3333333333334\" cy=\"519.5\"/><circle r=\"10\" class=\"node\" id=\"0_7\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"829.3333333333334\" cy=\"559.5\"/><circle r=\"10\" class=\"node\" id=\"0_8\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"829.3333333333334\" cy=\"599.5\"/><circle r=\"10\" class=\"node\" id=\"0_9\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"829.3333333333334\" cy=\"639.5\"/><circle r=\"10\" class=\"node\" id=\"1_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1009.3333333333334\" cy=\"439.5\"/><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"794.3333333333334\" y=\"679.5\">Input Layer ∈ ℝ¹⁰</text><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"974.3333333333334\" y=\"679.5\">Output Layer ∈ ℝ²</text><circle r=\"10\" class=\"node\" id=\"1_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1009.3333333333334\" cy=\"479.5\"/></g><defs><marker id=\"arrow\" viewBox=\"0 -5 10 10\" markerWidth=\"7\" markerHeight=\"7\" orient=\"auto\" refX=\"40\"><path d=\"M0,-5L10,0L0,5\" style=\"stroke: rgb(80, 80, 80); fill: none;\"/></marker></defs></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import SVG\n",
    "SVG(filename='assets/nn-0-hidden.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous structure was modeled using TensorFlow with the code of the following cell. I simply listed the layers in an ordered manner, and for each I specified the shape of its output as well as their activation function. For the MLPs I only used Dense layers, but you will see later on that other types of NNs use other layers. Note that the input layer does not need to be declared, given that its shape is inferred from the input data. The output shape of the output layer must match the number of response variables we are trying to predict. Finally, I used the sigmoid activation function for the output layer since the expected outputs should be in the range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1314 - accuracy: 0.8434 - mean_squared_error: 0.1314 - val_loss: 0.0945 - val_accuracy: 0.8912 - val_mean_squared_error: 0.0945\n",
      "Epoch 2/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0904 - accuracy: 0.8930 - mean_squared_error: 0.0904 - val_loss: 0.0838 - val_accuracy: 0.8959 - val_mean_squared_error: 0.0838\n",
      "Epoch 3/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0834 - accuracy: 0.8968 - mean_squared_error: 0.0834 - val_loss: 0.0794 - val_accuracy: 0.8975 - val_mean_squared_error: 0.0794\n",
      "Epoch 4/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0799 - accuracy: 0.8991 - mean_squared_error: 0.0799 - val_loss: 0.0769 - val_accuracy: 0.9005 - val_mean_squared_error: 0.0769\n",
      "Epoch 5/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0778 - accuracy: 0.9006 - mean_squared_error: 0.0778 - val_loss: 0.0752 - val_accuracy: 0.9025 - val_mean_squared_error: 0.0752\n",
      "Epoch 6/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0764 - accuracy: 0.9019 - mean_squared_error: 0.0764 - val_loss: 0.0740 - val_accuracy: 0.9039 - val_mean_squared_error: 0.0740\n",
      "Epoch 7/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0753 - accuracy: 0.9024 - mean_squared_error: 0.0753 - val_loss: 0.0730 - val_accuracy: 0.9043 - val_mean_squared_error: 0.0730\n",
      "Epoch 8/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0745 - accuracy: 0.9032 - mean_squared_error: 0.0745 - val_loss: 0.0723 - val_accuracy: 0.9049 - val_mean_squared_error: 0.0723\n",
      "Epoch 9/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0738 - accuracy: 0.9035 - mean_squared_error: 0.0738 - val_loss: 0.0717 - val_accuracy: 0.9060 - val_mean_squared_error: 0.0717\n",
      "Epoch 10/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0732 - accuracy: 0.9041 - mean_squared_error: 0.0732 - val_loss: 0.0712 - val_accuracy: 0.9065 - val_mean_squared_error: 0.0712\n",
      "Epoch 11/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0727 - accuracy: 0.9046 - mean_squared_error: 0.0727 - val_loss: 0.0708 - val_accuracy: 0.9063 - val_mean_squared_error: 0.0708\n",
      "Epoch 12/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0723 - accuracy: 0.9051 - mean_squared_error: 0.0723 - val_loss: 0.0703 - val_accuracy: 0.9061 - val_mean_squared_error: 0.0703\n",
      "Epoch 13/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0719 - accuracy: 0.9055 - mean_squared_error: 0.0719 - val_loss: 0.0700 - val_accuracy: 0.9068 - val_mean_squared_error: 0.0700\n",
      "Epoch 14/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0716 - accuracy: 0.9055 - mean_squared_error: 0.0716 - val_loss: 0.0697 - val_accuracy: 0.9063 - val_mean_squared_error: 0.0697\n",
      "Epoch 15/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0713 - accuracy: 0.9062 - mean_squared_error: 0.0713 - val_loss: 0.0694 - val_accuracy: 0.9069 - val_mean_squared_error: 0.0694\n",
      "Epoch 16/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0710 - accuracy: 0.9065 - mean_squared_error: 0.0710 - val_loss: 0.0692 - val_accuracy: 0.9069 - val_mean_squared_error: 0.0692\n",
      "Epoch 17/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9070 - mean_squared_error: 0.0707 - val_loss: 0.0689 - val_accuracy: 0.9083 - val_mean_squared_error: 0.0689\n",
      "Epoch 18/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0705 - accuracy: 0.9072 - mean_squared_error: 0.0705 - val_loss: 0.0687 - val_accuracy: 0.9093 - val_mean_squared_error: 0.0687\n",
      "Epoch 19/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0703 - accuracy: 0.9076 - mean_squared_error: 0.0703 - val_loss: 0.0685 - val_accuracy: 0.9097 - val_mean_squared_error: 0.0685\n",
      "Epoch 20/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0701 - accuracy: 0.9077 - mean_squared_error: 0.0701 - val_loss: 0.0683 - val_accuracy: 0.9096 - val_mean_squared_error: 0.0683\n",
      "Epoch 21/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0699 - accuracy: 0.9078 - mean_squared_error: 0.0699 - val_loss: 0.0681 - val_accuracy: 0.9096 - val_mean_squared_error: 0.0681\n",
      "Epoch 22/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0697 - accuracy: 0.9083 - mean_squared_error: 0.0697 - val_loss: 0.0680 - val_accuracy: 0.9092 - val_mean_squared_error: 0.0680\n",
      "Epoch 23/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0695 - accuracy: 0.9083 - mean_squared_error: 0.0695 - val_loss: 0.0678 - val_accuracy: 0.9100 - val_mean_squared_error: 0.0678\n",
      "Epoch 24/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0693 - accuracy: 0.9086 - mean_squared_error: 0.0693 - val_loss: 0.0676 - val_accuracy: 0.9104 - val_mean_squared_error: 0.0676\n",
      "Epoch 25/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0691 - accuracy: 0.9091 - mean_squared_error: 0.0691 - val_loss: 0.0675 - val_accuracy: 0.9103 - val_mean_squared_error: 0.0675\n",
      "Epoch 26/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0690 - accuracy: 0.9091 - mean_squared_error: 0.0690 - val_loss: 0.0673 - val_accuracy: 0.9111 - val_mean_squared_error: 0.0673\n",
      "Epoch 27/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0688 - accuracy: 0.9097 - mean_squared_error: 0.0688 - val_loss: 0.0672 - val_accuracy: 0.9112 - val_mean_squared_error: 0.0672\n",
      "Epoch 28/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0687 - accuracy: 0.9095 - mean_squared_error: 0.0687 - val_loss: 0.0670 - val_accuracy: 0.9115 - val_mean_squared_error: 0.0670\n",
      "Epoch 29/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0685 - accuracy: 0.9098 - mean_squared_error: 0.0685 - val_loss: 0.0669 - val_accuracy: 0.9120 - val_mean_squared_error: 0.0669\n",
      "Epoch 30/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0684 - accuracy: 0.9103 - mean_squared_error: 0.0684 - val_loss: 0.0667 - val_accuracy: 0.9119 - val_mean_squared_error: 0.0667\n",
      "Epoch 31/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0683 - accuracy: 0.9099 - mean_squared_error: 0.0683 - val_loss: 0.0666 - val_accuracy: 0.9115 - val_mean_squared_error: 0.0666\n",
      "Epoch 32/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0681 - accuracy: 0.9104 - mean_squared_error: 0.0681 - val_loss: 0.0665 - val_accuracy: 0.9121 - val_mean_squared_error: 0.0665\n",
      "Epoch 33/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0680 - accuracy: 0.9106 - mean_squared_error: 0.0680 - val_loss: 0.0664 - val_accuracy: 0.9120 - val_mean_squared_error: 0.0664\n",
      "Epoch 34/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0679 - accuracy: 0.9107 - mean_squared_error: 0.0679 - val_loss: 0.0663 - val_accuracy: 0.9123 - val_mean_squared_error: 0.0663\n",
      "Epoch 35/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0677 - accuracy: 0.9110 - mean_squared_error: 0.0677 - val_loss: 0.0661 - val_accuracy: 0.9120 - val_mean_squared_error: 0.0661\n",
      "Epoch 36/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0676 - accuracy: 0.9113 - mean_squared_error: 0.0676 - val_loss: 0.0660 - val_accuracy: 0.9121 - val_mean_squared_error: 0.0660\n",
      "Epoch 37/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0675 - accuracy: 0.9113 - mean_squared_error: 0.0675 - val_loss: 0.0659 - val_accuracy: 0.9124 - val_mean_squared_error: 0.0659\n",
      "Epoch 38/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0674 - accuracy: 0.9113 - mean_squared_error: 0.0674 - val_loss: 0.0658 - val_accuracy: 0.9124 - val_mean_squared_error: 0.0658\n",
      "Epoch 39/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0673 - accuracy: 0.9113 - mean_squared_error: 0.0673 - val_loss: 0.0657 - val_accuracy: 0.9125 - val_mean_squared_error: 0.0657\n",
      "Epoch 40/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0672 - accuracy: 0.9115 - mean_squared_error: 0.0672 - val_loss: 0.0656 - val_accuracy: 0.9127 - val_mean_squared_error: 0.0656\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0670 - accuracy: 0.9117 - mean_squared_error: 0.0670 - val_loss: 0.0655 - val_accuracy: 0.9128 - val_mean_squared_error: 0.0655\n",
      "Epoch 42/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0669 - accuracy: 0.9121 - mean_squared_error: 0.0669 - val_loss: 0.0654 - val_accuracy: 0.9129 - val_mean_squared_error: 0.0654\n",
      "Epoch 43/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0668 - accuracy: 0.9122 - mean_squared_error: 0.0668 - val_loss: 0.0653 - val_accuracy: 0.9135 - val_mean_squared_error: 0.0653\n",
      "Epoch 44/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0667 - accuracy: 0.9120 - mean_squared_error: 0.0667 - val_loss: 0.0652 - val_accuracy: 0.9139 - val_mean_squared_error: 0.0652\n",
      "Epoch 45/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0666 - accuracy: 0.9122 - mean_squared_error: 0.0666 - val_loss: 0.0651 - val_accuracy: 0.9139 - val_mean_squared_error: 0.0651\n",
      "Epoch 46/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0665 - accuracy: 0.9124 - mean_squared_error: 0.0665 - val_loss: 0.0650 - val_accuracy: 0.9145 - val_mean_squared_error: 0.0650\n",
      "Epoch 47/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0664 - accuracy: 0.9124 - mean_squared_error: 0.0664 - val_loss: 0.0649 - val_accuracy: 0.9151 - val_mean_squared_error: 0.0649\n",
      "Epoch 48/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0663 - accuracy: 0.9125 - mean_squared_error: 0.0663 - val_loss: 0.0648 - val_accuracy: 0.9149 - val_mean_squared_error: 0.0648\n",
      "Epoch 49/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0662 - accuracy: 0.9128 - mean_squared_error: 0.0662 - val_loss: 0.0647 - val_accuracy: 0.9153 - val_mean_squared_error: 0.0647\n",
      "Epoch 50/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0661 - accuracy: 0.9131 - mean_squared_error: 0.0661 - val_loss: 0.0646 - val_accuracy: 0.9153 - val_mean_squared_error: 0.0646\n",
      "\n",
      "accuracy: 0.9127869009971619 | mean_squared_error: 0.06555786728858948\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    # output layer\n",
    "    Dense(2, activation='sigmoid'),\n",
    "])\n",
    "train_and_evaluate_model(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training method prints the performance of the model after each epoch, as well as the final performance metrics obtained using the test set. For this exercise, I looked at the accuracy of the model (the higher the better) and its mean squared error (the lower the better) to assess its performance. At this point, it was hard to say whether the model was doing well or not because I had nothing to compare it to, but +90% of accuracy didn't look bad. However, I knew that there was a big margin of improvement.\n",
    "\n",
    "I then added the first hidden layer to the model. The previous model worked similarly to a logistic regression, since the structure didn't allow the model to learn non-linear relations between the predictor variables and the response variables. Adding a hidden layer fixed that. The new structure looked like the following diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" style=\"cursor: move;\" width=\"1872\" height=\"939\"><g transform=\"translate(-453.4407967506179,-189.33900437484783) scale(1.3717332885905962)\"><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,279.5, 956,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,319.5, 956,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,359.5, 956,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,399.5, 956,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,439.5, 956,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,479.5, 956,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,519.5, 956,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,559.5, 956,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,599.5, 956,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,639.5, 956,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,279.5, 1136,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,279.5, 956,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,319.5, 956,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,359.5, 956,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,399.5, 956,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,439.5, 956,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,479.5, 956,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,519.5, 956,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,559.5, 956,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,599.5, 956,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,639.5, 956,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,319.5, 1136,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,279.5, 956,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,319.5, 956,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,359.5, 956,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,399.5, 956,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,439.5, 956,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,479.5, 956,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,519.5, 956,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,559.5, 956,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,599.5, 956,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,639.5, 956,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,359.5, 1136,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,279.5, 956,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,319.5, 956,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,359.5, 956,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,399.5, 956,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,439.5, 956,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,479.5, 956,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,519.5, 956,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,559.5, 956,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,599.5, 956,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,639.5, 956,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,399.5, 1136,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,279.5, 956,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,319.5, 956,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,359.5, 956,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,399.5, 956,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,439.5, 956,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,479.5, 956,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,519.5, 956,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,559.5, 956,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,599.5, 956,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,639.5, 956,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,439.5, 1136,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,279.5, 956,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,319.5, 956,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,359.5, 956,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,399.5, 956,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,439.5, 956,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,479.5, 956,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,519.5, 956,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,559.5, 956,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,599.5, 956,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,639.5, 956,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,479.5, 1136,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,279.5, 956,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,319.5, 956,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,359.5, 956,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,399.5, 956,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,439.5, 956,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,479.5, 956,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,519.5, 956,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,559.5, 956,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,599.5, 956,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,639.5, 956,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,519.5, 1136,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,279.5, 956,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,319.5, 956,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,359.5, 956,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,399.5, 956,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,439.5, 956,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,479.5, 956,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,519.5, 956,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,559.5, 956,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,599.5, 956,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,639.5, 956,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,559.5, 1136,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,279.5, 956,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,319.5, 956,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,359.5, 956,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,399.5, 956,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,439.5, 956,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,479.5, 956,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,519.5, 956,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,559.5, 956,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,599.5, 956,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,639.5, 956,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,599.5, 1136,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,279.5, 956,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,319.5, 956,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,359.5, 956,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,399.5, 956,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,439.5, 956,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,479.5, 956,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,519.5, 956,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,559.5, 956,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,599.5, 956,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M776,639.5, 956,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,639.5, 1136,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,279.5, 1136,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,319.5, 1136,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,359.5, 1136,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,399.5, 1136,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,439.5, 1136,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,479.5, 1136,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,519.5, 1136,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,559.5, 1136,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,599.5, 1136,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M956,639.5, 1136,479.5\"/><circle r=\"10\" class=\"node\" id=\"0_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"776\" cy=\"279.5\"/><circle r=\"10\" class=\"node\" id=\"0_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"776\" cy=\"319.5\"/><circle r=\"10\" class=\"node\" id=\"0_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"776\" cy=\"359.5\"/><circle r=\"10\" class=\"node\" id=\"0_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"776\" cy=\"399.5\"/><circle r=\"10\" class=\"node\" id=\"0_4\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"776\" cy=\"439.5\"/><circle r=\"10\" class=\"node\" id=\"0_5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"776\" cy=\"479.5\"/><circle r=\"10\" class=\"node\" id=\"0_6\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"776\" cy=\"519.5\"/><circle r=\"10\" class=\"node\" id=\"0_7\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"776\" cy=\"559.5\"/><circle r=\"10\" class=\"node\" id=\"0_8\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"776\" cy=\"599.5\"/><circle r=\"10\" class=\"node\" id=\"0_9\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"776\" cy=\"639.5\"/><circle r=\"10\" class=\"node\" id=\"1_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"956\" cy=\"279.5\"/><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"741\" y=\"679.5\">Input Layer ∈ ℝ¹⁰</text><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"921\" y=\"679.5\">Hidden Layer ∈ ℝ¹⁰</text><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"1101\" y=\"679.5\">Output Layer ∈ ℝ²</text><circle r=\"10\" class=\"node\" id=\"1_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"956\" cy=\"319.5\"/><circle r=\"10\" class=\"node\" id=\"1_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"956\" cy=\"359.5\"/><circle r=\"10\" class=\"node\" id=\"1_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"956\" cy=\"399.5\"/><circle r=\"10\" class=\"node\" id=\"1_4\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"956\" cy=\"439.5\"/><circle r=\"10\" class=\"node\" id=\"1_5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"956\" cy=\"479.5\"/><circle r=\"10\" class=\"node\" id=\"1_6\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"956\" cy=\"519.5\"/><circle r=\"10\" class=\"node\" id=\"1_7\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"956\" cy=\"559.5\"/><circle r=\"10\" class=\"node\" id=\"1_8\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"956\" cy=\"599.5\"/><circle r=\"10\" class=\"node\" id=\"1_9\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"956\" cy=\"639.5\"/><circle r=\"10\" class=\"node\" id=\"2_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1136\" cy=\"439.5\"/><circle r=\"10\" class=\"node\" id=\"2_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1136\" cy=\"479.5\"/></g><defs><marker id=\"arrow\" viewBox=\"0 -5 10 10\" markerWidth=\"7\" markerHeight=\"7\" orient=\"auto\" refX=\"40\"><path d=\"M0,-5L10,0L0,5\" style=\"stroke: rgb(80, 80, 80); fill: none;\"/></marker></defs></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import SVG\n",
    "SVG(filename='assets/nn-1-hidden.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of neurons of the hidden layer was set to 10 (the same as the input layer). As mentioned in the introduction, I did not try many possible combinations of structures as it would have taken too much time. Keeping the number of neurons the same as the previous layer tends to yield good results. For now, I also used the sigmoid as the activation function of the hidden layer. I created the new model using the previous one as an example, and trained and evaluated it as before. It is important to note that the hidden layer was added before the output, and not after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2069 - accuracy: 0.6780 - mean_squared_error: 0.2069 - val_loss: 0.1781 - val_accuracy: 0.7038 - val_mean_squared_error: 0.1781\n",
      "Epoch 2/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1505 - accuracy: 0.7918 - mean_squared_error: 0.1505 - val_loss: 0.1217 - val_accuracy: 0.8652 - val_mean_squared_error: 0.1217\n",
      "Epoch 3/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1082 - accuracy: 0.8759 - mean_squared_error: 0.1082 - val_loss: 0.0945 - val_accuracy: 0.8888 - val_mean_squared_error: 0.0945\n",
      "Epoch 4/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0905 - accuracy: 0.8922 - mean_squared_error: 0.0905 - val_loss: 0.0839 - val_accuracy: 0.8969 - val_mean_squared_error: 0.0839\n",
      "Epoch 5/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0830 - accuracy: 0.8974 - mean_squared_error: 0.0830 - val_loss: 0.0787 - val_accuracy: 0.8993 - val_mean_squared_error: 0.0787\n",
      "Epoch 6/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0791 - accuracy: 0.8996 - mean_squared_error: 0.0791 - val_loss: 0.0758 - val_accuracy: 0.9029 - val_mean_squared_error: 0.0758\n",
      "Epoch 7/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0768 - accuracy: 0.9017 - mean_squared_error: 0.0768 - val_loss: 0.0739 - val_accuracy: 0.9039 - val_mean_squared_error: 0.0739\n",
      "Epoch 8/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0753 - accuracy: 0.9024 - mean_squared_error: 0.0753 - val_loss: 0.0727 - val_accuracy: 0.9035 - val_mean_squared_error: 0.0727\n",
      "Epoch 9/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0742 - accuracy: 0.9031 - mean_squared_error: 0.0742 - val_loss: 0.0717 - val_accuracy: 0.9055 - val_mean_squared_error: 0.0717\n",
      "Epoch 10/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0733 - accuracy: 0.9032 - mean_squared_error: 0.0733 - val_loss: 0.0710 - val_accuracy: 0.9073 - val_mean_squared_error: 0.0710\n",
      "Epoch 11/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0726 - accuracy: 0.9040 - mean_squared_error: 0.0726 - val_loss: 0.0704 - val_accuracy: 0.9063 - val_mean_squared_error: 0.0704\n",
      "Epoch 12/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0721 - accuracy: 0.9046 - mean_squared_error: 0.0721 - val_loss: 0.0698 - val_accuracy: 0.9076 - val_mean_squared_error: 0.0698\n",
      "Epoch 13/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0716 - accuracy: 0.9051 - mean_squared_error: 0.0716 - val_loss: 0.0694 - val_accuracy: 0.9087 - val_mean_squared_error: 0.0694\n",
      "Epoch 14/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0712 - accuracy: 0.9056 - mean_squared_error: 0.0712 - val_loss: 0.0690 - val_accuracy: 0.9084 - val_mean_squared_error: 0.0690\n",
      "Epoch 15/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0708 - accuracy: 0.9058 - mean_squared_error: 0.0708 - val_loss: 0.0687 - val_accuracy: 0.9101 - val_mean_squared_error: 0.0687\n",
      "Epoch 16/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0704 - accuracy: 0.9062 - mean_squared_error: 0.0704 - val_loss: 0.0684 - val_accuracy: 0.9097 - val_mean_squared_error: 0.0684\n",
      "Epoch 17/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0701 - accuracy: 0.9063 - mean_squared_error: 0.0701 - val_loss: 0.0681 - val_accuracy: 0.9107 - val_mean_squared_error: 0.0681\n",
      "Epoch 18/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0698 - accuracy: 0.9072 - mean_squared_error: 0.0698 - val_loss: 0.0678 - val_accuracy: 0.9109 - val_mean_squared_error: 0.0678\n",
      "Epoch 19/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0695 - accuracy: 0.9072 - mean_squared_error: 0.0695 - val_loss: 0.0675 - val_accuracy: 0.9107 - val_mean_squared_error: 0.0675\n",
      "Epoch 20/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0692 - accuracy: 0.9080 - mean_squared_error: 0.0692 - val_loss: 0.0672 - val_accuracy: 0.9107 - val_mean_squared_error: 0.0672\n",
      "Epoch 21/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0690 - accuracy: 0.9081 - mean_squared_error: 0.0690 - val_loss: 0.0670 - val_accuracy: 0.9113 - val_mean_squared_error: 0.0670\n",
      "Epoch 22/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0687 - accuracy: 0.9086 - mean_squared_error: 0.0687 - val_loss: 0.0668 - val_accuracy: 0.9120 - val_mean_squared_error: 0.0668\n",
      "Epoch 23/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0684 - accuracy: 0.9090 - mean_squared_error: 0.0684 - val_loss: 0.0665 - val_accuracy: 0.9120 - val_mean_squared_error: 0.0665\n",
      "Epoch 24/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0682 - accuracy: 0.9092 - mean_squared_error: 0.0682 - val_loss: 0.0663 - val_accuracy: 0.9128 - val_mean_squared_error: 0.0663\n",
      "Epoch 25/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0679 - accuracy: 0.9093 - mean_squared_error: 0.0679 - val_loss: 0.0661 - val_accuracy: 0.9123 - val_mean_squared_error: 0.0661\n",
      "Epoch 26/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0677 - accuracy: 0.9101 - mean_squared_error: 0.0677 - val_loss: 0.0658 - val_accuracy: 0.9133 - val_mean_squared_error: 0.0658\n",
      "Epoch 27/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0674 - accuracy: 0.9099 - mean_squared_error: 0.0674 - val_loss: 0.0656 - val_accuracy: 0.9135 - val_mean_squared_error: 0.0656\n",
      "Epoch 28/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0671 - accuracy: 0.9107 - mean_squared_error: 0.0671 - val_loss: 0.0654 - val_accuracy: 0.9137 - val_mean_squared_error: 0.0654\n",
      "Epoch 29/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0669 - accuracy: 0.9113 - mean_squared_error: 0.0669 - val_loss: 0.0651 - val_accuracy: 0.9132 - val_mean_squared_error: 0.0651\n",
      "Epoch 30/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0667 - accuracy: 0.9113 - mean_squared_error: 0.0667 - val_loss: 0.0649 - val_accuracy: 0.9141 - val_mean_squared_error: 0.0649\n",
      "Epoch 31/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0664 - accuracy: 0.9116 - mean_squared_error: 0.0664 - val_loss: 0.0647 - val_accuracy: 0.9145 - val_mean_squared_error: 0.0647\n",
      "Epoch 32/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0661 - accuracy: 0.9119 - mean_squared_error: 0.0661 - val_loss: 0.0645 - val_accuracy: 0.9149 - val_mean_squared_error: 0.0645\n",
      "Epoch 33/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0659 - accuracy: 0.9121 - mean_squared_error: 0.0659 - val_loss: 0.0642 - val_accuracy: 0.9141 - val_mean_squared_error: 0.0642\n",
      "Epoch 34/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0657 - accuracy: 0.9124 - mean_squared_error: 0.0657 - val_loss: 0.0640 - val_accuracy: 0.9145 - val_mean_squared_error: 0.0640\n",
      "Epoch 35/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0654 - accuracy: 0.9129 - mean_squared_error: 0.0654 - val_loss: 0.0638 - val_accuracy: 0.9155 - val_mean_squared_error: 0.0638\n",
      "Epoch 36/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0652 - accuracy: 0.9129 - mean_squared_error: 0.0652 - val_loss: 0.0636 - val_accuracy: 0.9167 - val_mean_squared_error: 0.0636\n",
      "Epoch 37/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0649 - accuracy: 0.9136 - mean_squared_error: 0.0649 - val_loss: 0.0633 - val_accuracy: 0.9156 - val_mean_squared_error: 0.0633\n",
      "Epoch 38/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0647 - accuracy: 0.9134 - mean_squared_error: 0.0647 - val_loss: 0.0631 - val_accuracy: 0.9165 - val_mean_squared_error: 0.0631\n",
      "Epoch 39/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0644 - accuracy: 0.9136 - mean_squared_error: 0.0644 - val_loss: 0.0629 - val_accuracy: 0.9171 - val_mean_squared_error: 0.0629\n",
      "Epoch 40/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0642 - accuracy: 0.9145 - mean_squared_error: 0.0642 - val_loss: 0.0626 - val_accuracy: 0.9165 - val_mean_squared_error: 0.0626\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0639 - accuracy: 0.9148 - mean_squared_error: 0.0639 - val_loss: 0.0624 - val_accuracy: 0.9167 - val_mean_squared_error: 0.0624\n",
      "Epoch 42/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0636 - accuracy: 0.9154 - mean_squared_error: 0.0636 - val_loss: 0.0622 - val_accuracy: 0.9168 - val_mean_squared_error: 0.0622\n",
      "Epoch 43/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0634 - accuracy: 0.9156 - mean_squared_error: 0.0634 - val_loss: 0.0619 - val_accuracy: 0.9177 - val_mean_squared_error: 0.0619\n",
      "Epoch 44/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0632 - accuracy: 0.9156 - mean_squared_error: 0.0632 - val_loss: 0.0617 - val_accuracy: 0.9181 - val_mean_squared_error: 0.0617\n",
      "Epoch 45/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0629 - accuracy: 0.9163 - mean_squared_error: 0.0629 - val_loss: 0.0615 - val_accuracy: 0.9183 - val_mean_squared_error: 0.0615\n",
      "Epoch 46/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0627 - accuracy: 0.9163 - mean_squared_error: 0.0627 - val_loss: 0.0613 - val_accuracy: 0.9187 - val_mean_squared_error: 0.0613\n",
      "Epoch 47/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0624 - accuracy: 0.9169 - mean_squared_error: 0.0624 - val_loss: 0.0611 - val_accuracy: 0.9191 - val_mean_squared_error: 0.0611\n",
      "Epoch 48/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0622 - accuracy: 0.9171 - mean_squared_error: 0.0622 - val_loss: 0.0609 - val_accuracy: 0.9196 - val_mean_squared_error: 0.0609\n",
      "Epoch 49/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0619 - accuracy: 0.9171 - mean_squared_error: 0.0619 - val_loss: 0.0606 - val_accuracy: 0.9205 - val_mean_squared_error: 0.0606\n",
      "Epoch 50/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0617 - accuracy: 0.9177 - mean_squared_error: 0.0617 - val_loss: 0.0604 - val_accuracy: 0.9211 - val_mean_squared_error: 0.0604\n",
      "\n",
      "accuracy: 0.9204201102256775 | mean_squared_error: 0.061129629611968994\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(10, activation='sigmoid'),\n",
    "    # output layer\n",
    "    Dense(2, activation='sigmoid'),\n",
    "])\n",
    "train_and_evaluate_model(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the improvement I achieved compared to the previous and simpler model. But I didn't stop there. I then focused on the activation functions. Each function used has different properties, and apart from adding non-linearity to the model predictions, they regulate the range of the output produced by the layer. But on the back-propagation stage they also affect the size of the correction applied to the weights in order to learn from the new data. This correction is known as the gradient, which corresponds to the derivative of the function with respect to the weights. Which means that the steeper the function (the higher the slope) the higher will be their gradients. With larger gradients, I would need fewer steps to reach the optimal solution, which would reduce the needed time to reach loss convergence. The disadvantage of taking big steps might be that I skip the best solution, but there are other parameters such as the learning rate that help to minimize this problem. The following plot compares the sigmoid and relu activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b5497bb850>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigmoid = lambda x: [1 / (1 + math.exp(-item)) for item in x]\n",
    "tanh = lambda x: np.tanh(x)\n",
    "\n",
    "x_values = np.linspace(-2, 2, 1000)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7), dpi=100)\n",
    "ax.axhline(0, ls='--', c=\"gray\", alpha=0.5)\n",
    "ax.axvline(0, ls='--', c=\"gray\", alpha=0.5)\n",
    "ax.plot(x_values, sigmoid(x_values), label=\"sigmoid\")\n",
    "ax.plot(x_values, tanh(x_values), label=\"tanh\")\n",
    "ax.set_title(\"Activation functions: sigmoid vs tanh\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear to see that the tanh is much steeper than the sigmoid. So I decided to try using this function as the activation function of the hidden layer instead of the sigmoid. However, I didn't use it on the output layer as it would reduce the performance of the model. The reason is that the predictions produced by the tanh function will be in the range [-1, 1], which is different than the range we expect [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1337 - accuracy: 0.8311 - mean_squared_error: 0.1337 - val_loss: 0.0815 - val_accuracy: 0.9000 - val_mean_squared_error: 0.0815\n",
      "Epoch 2/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0777 - accuracy: 0.9005 - mean_squared_error: 0.0777 - val_loss: 0.0723 - val_accuracy: 0.9064 - val_mean_squared_error: 0.0723\n",
      "Epoch 3/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0728 - accuracy: 0.9044 - mean_squared_error: 0.0728 - val_loss: 0.0697 - val_accuracy: 0.9069 - val_mean_squared_error: 0.0697\n",
      "Epoch 4/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0708 - accuracy: 0.9059 - mean_squared_error: 0.0708 - val_loss: 0.0683 - val_accuracy: 0.9107 - val_mean_squared_error: 0.0683\n",
      "Epoch 5/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0695 - accuracy: 0.9069 - mean_squared_error: 0.0695 - val_loss: 0.0671 - val_accuracy: 0.9105 - val_mean_squared_error: 0.0671\n",
      "Epoch 6/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0684 - accuracy: 0.9082 - mean_squared_error: 0.0684 - val_loss: 0.0662 - val_accuracy: 0.9111 - val_mean_squared_error: 0.0662\n",
      "Epoch 7/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0673 - accuracy: 0.9105 - mean_squared_error: 0.0673 - val_loss: 0.0652 - val_accuracy: 0.9135 - val_mean_squared_error: 0.0652\n",
      "Epoch 8/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0663 - accuracy: 0.9113 - mean_squared_error: 0.0663 - val_loss: 0.0643 - val_accuracy: 0.9139 - val_mean_squared_error: 0.0643\n",
      "Epoch 9/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0653 - accuracy: 0.9134 - mean_squared_error: 0.0653 - val_loss: 0.0634 - val_accuracy: 0.9151 - val_mean_squared_error: 0.0634\n",
      "Epoch 10/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0643 - accuracy: 0.9138 - mean_squared_error: 0.0643 - val_loss: 0.0625 - val_accuracy: 0.9169 - val_mean_squared_error: 0.0625\n",
      "Epoch 11/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0634 - accuracy: 0.9152 - mean_squared_error: 0.0634 - val_loss: 0.0616 - val_accuracy: 0.9187 - val_mean_squared_error: 0.0616\n",
      "Epoch 12/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0624 - accuracy: 0.9173 - mean_squared_error: 0.0624 - val_loss: 0.0608 - val_accuracy: 0.9189 - val_mean_squared_error: 0.0608\n",
      "Epoch 13/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0614 - accuracy: 0.9185 - mean_squared_error: 0.0614 - val_loss: 0.0601 - val_accuracy: 0.9204 - val_mean_squared_error: 0.0601\n",
      "Epoch 14/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0605 - accuracy: 0.9194 - mean_squared_error: 0.0605 - val_loss: 0.0594 - val_accuracy: 0.9228 - val_mean_squared_error: 0.0594\n",
      "Epoch 15/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0596 - accuracy: 0.9210 - mean_squared_error: 0.0596 - val_loss: 0.0586 - val_accuracy: 0.9235 - val_mean_squared_error: 0.0586\n",
      "Epoch 16/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0588 - accuracy: 0.9222 - mean_squared_error: 0.0588 - val_loss: 0.0577 - val_accuracy: 0.9249 - val_mean_squared_error: 0.0577\n",
      "Epoch 17/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0580 - accuracy: 0.9233 - mean_squared_error: 0.0580 - val_loss: 0.0571 - val_accuracy: 0.9255 - val_mean_squared_error: 0.0571\n",
      "Epoch 18/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0572 - accuracy: 0.9245 - mean_squared_error: 0.0572 - val_loss: 0.0566 - val_accuracy: 0.9265 - val_mean_squared_error: 0.0566\n",
      "Epoch 19/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0566 - accuracy: 0.9257 - mean_squared_error: 0.0566 - val_loss: 0.0559 - val_accuracy: 0.9291 - val_mean_squared_error: 0.0559\n",
      "Epoch 20/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0559 - accuracy: 0.9270 - mean_squared_error: 0.0559 - val_loss: 0.0554 - val_accuracy: 0.9280 - val_mean_squared_error: 0.0554\n",
      "Epoch 21/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0554 - accuracy: 0.9270 - mean_squared_error: 0.0554 - val_loss: 0.0550 - val_accuracy: 0.9276 - val_mean_squared_error: 0.0550\n",
      "Epoch 22/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0549 - accuracy: 0.9287 - mean_squared_error: 0.0549 - val_loss: 0.0545 - val_accuracy: 0.9289 - val_mean_squared_error: 0.0545\n",
      "Epoch 23/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0544 - accuracy: 0.9288 - mean_squared_error: 0.0544 - val_loss: 0.0544 - val_accuracy: 0.9291 - val_mean_squared_error: 0.0544\n",
      "Epoch 24/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0540 - accuracy: 0.9293 - mean_squared_error: 0.0540 - val_loss: 0.0538 - val_accuracy: 0.9299 - val_mean_squared_error: 0.0538\n",
      "Epoch 25/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0536 - accuracy: 0.9296 - mean_squared_error: 0.0536 - val_loss: 0.0535 - val_accuracy: 0.9301 - val_mean_squared_error: 0.0535\n",
      "Epoch 26/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0533 - accuracy: 0.9302 - mean_squared_error: 0.0533 - val_loss: 0.0531 - val_accuracy: 0.9313 - val_mean_squared_error: 0.0531\n",
      "Epoch 27/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0528 - accuracy: 0.9306 - mean_squared_error: 0.0528 - val_loss: 0.0529 - val_accuracy: 0.9304 - val_mean_squared_error: 0.0529\n",
      "Epoch 28/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0524 - accuracy: 0.9317 - mean_squared_error: 0.0524 - val_loss: 0.0524 - val_accuracy: 0.9324 - val_mean_squared_error: 0.0524\n",
      "Epoch 29/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0521 - accuracy: 0.9319 - mean_squared_error: 0.0521 - val_loss: 0.0520 - val_accuracy: 0.9328 - val_mean_squared_error: 0.0520\n",
      "Epoch 30/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0518 - accuracy: 0.9325 - mean_squared_error: 0.0518 - val_loss: 0.0518 - val_accuracy: 0.9324 - val_mean_squared_error: 0.0518\n",
      "Epoch 31/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0514 - accuracy: 0.9334 - mean_squared_error: 0.0514 - val_loss: 0.0516 - val_accuracy: 0.9321 - val_mean_squared_error: 0.0516\n",
      "Epoch 32/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0511 - accuracy: 0.9331 - mean_squared_error: 0.0511 - val_loss: 0.0512 - val_accuracy: 0.9333 - val_mean_squared_error: 0.0512\n",
      "Epoch 33/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0508 - accuracy: 0.9334 - mean_squared_error: 0.0508 - val_loss: 0.0511 - val_accuracy: 0.9333 - val_mean_squared_error: 0.0511\n",
      "Epoch 34/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0506 - accuracy: 0.9337 - mean_squared_error: 0.0506 - val_loss: 0.0508 - val_accuracy: 0.9336 - val_mean_squared_error: 0.0508\n",
      "Epoch 35/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0503 - accuracy: 0.9342 - mean_squared_error: 0.0503 - val_loss: 0.0508 - val_accuracy: 0.9336 - val_mean_squared_error: 0.0508\n",
      "Epoch 36/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0501 - accuracy: 0.9341 - mean_squared_error: 0.0501 - val_loss: 0.0503 - val_accuracy: 0.9348 - val_mean_squared_error: 0.0503\n",
      "Epoch 37/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0499 - accuracy: 0.9347 - mean_squared_error: 0.0499 - val_loss: 0.0501 - val_accuracy: 0.9352 - val_mean_squared_error: 0.0501\n",
      "Epoch 38/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0497 - accuracy: 0.9354 - mean_squared_error: 0.0497 - val_loss: 0.0501 - val_accuracy: 0.9340 - val_mean_squared_error: 0.0501\n",
      "Epoch 39/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0495 - accuracy: 0.9355 - mean_squared_error: 0.0495 - val_loss: 0.0498 - val_accuracy: 0.9363 - val_mean_squared_error: 0.0498\n",
      "Epoch 40/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0493 - accuracy: 0.9355 - mean_squared_error: 0.0493 - val_loss: 0.0499 - val_accuracy: 0.9356 - val_mean_squared_error: 0.0499\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0490 - accuracy: 0.9366 - mean_squared_error: 0.0490 - val_loss: 0.0494 - val_accuracy: 0.9361 - val_mean_squared_error: 0.0494\n",
      "Epoch 42/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0488 - accuracy: 0.9367 - mean_squared_error: 0.0488 - val_loss: 0.0494 - val_accuracy: 0.9372 - val_mean_squared_error: 0.0494\n",
      "Epoch 43/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0487 - accuracy: 0.9366 - mean_squared_error: 0.0487 - val_loss: 0.0493 - val_accuracy: 0.9365 - val_mean_squared_error: 0.0493\n",
      "Epoch 44/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0486 - accuracy: 0.9367 - mean_squared_error: 0.0486 - val_loss: 0.0489 - val_accuracy: 0.9375 - val_mean_squared_error: 0.0489\n",
      "Epoch 45/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0483 - accuracy: 0.9366 - mean_squared_error: 0.0483 - val_loss: 0.0488 - val_accuracy: 0.9383 - val_mean_squared_error: 0.0488\n",
      "Epoch 46/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0482 - accuracy: 0.9375 - mean_squared_error: 0.0482 - val_loss: 0.0487 - val_accuracy: 0.9380 - val_mean_squared_error: 0.0487\n",
      "Epoch 47/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0481 - accuracy: 0.9374 - mean_squared_error: 0.0481 - val_loss: 0.0485 - val_accuracy: 0.9381 - val_mean_squared_error: 0.0485\n",
      "Epoch 48/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0479 - accuracy: 0.9384 - mean_squared_error: 0.0479 - val_loss: 0.0485 - val_accuracy: 0.9387 - val_mean_squared_error: 0.0485\n",
      "Epoch 49/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0477 - accuracy: 0.9380 - mean_squared_error: 0.0477 - val_loss: 0.0483 - val_accuracy: 0.9389 - val_mean_squared_error: 0.0483\n",
      "Epoch 50/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0476 - accuracy: 0.9384 - mean_squared_error: 0.0476 - val_loss: 0.0481 - val_accuracy: 0.9387 - val_mean_squared_error: 0.0481\n",
      "\n",
      "accuracy: 0.938826322555542 | mean_squared_error: 0.047255609184503555\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(10, activation='tanh'),\n",
    "    # output layer\n",
    "    Dense(2, activation='sigmoid'),\n",
    "])\n",
    "train_and_evaluate_model(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the large improvement obtained. The next configuration to modify will be the loss function. The default we are using right now is the mean squared error, which is not the best when we know that the output corresponds to probabilities. On the other hand, the Cross Entropy Loss works very well for this problem, given that the loss values produced for predictions close the incorrect value increase exponentially towards infinity. The mean squared error's maximum loss is 1 for probabilities. Larger loss values will also produce larger gradients. Try adding as the last parameter passed to the train_and_evaluate_model method the following: loss=\"binary_crossentropy\".\n",
    "\n",
    "I want to point out the large improvement obtained. The next configuration I will modify is the loss function. The default being used right now is the mean squared error, which is not the best when we know that the output corresponds to probabilities. On the other hand, the Cross Entropy Loss works very well for this problem, given that the loss values produced for predictions close the incorrect value increase exponentially towards infinity. The mean squared error's maximum loss is 1 for probabilities. Larger loss values will also produce larger gradients. I added as the last parameter passed to the train_and_evaluate_model method the following: loss=\"binary_crossentropy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3301 - accuracy: 0.8667 - mean_squared_error: 0.1007 - val_loss: 0.2414 - val_accuracy: 0.9051 - val_mean_squared_error: 0.0703\n",
      "Epoch 2/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2392 - accuracy: 0.9050 - mean_squared_error: 0.0704 - val_loss: 0.2300 - val_accuracy: 0.9121 - val_mean_squared_error: 0.0667\n",
      "Epoch 3/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2286 - accuracy: 0.9099 - mean_squared_error: 0.0673 - val_loss: 0.2219 - val_accuracy: 0.9139 - val_mean_squared_error: 0.0642\n",
      "Epoch 4/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2190 - accuracy: 0.9133 - mean_squared_error: 0.0643 - val_loss: 0.2137 - val_accuracy: 0.9167 - val_mean_squared_error: 0.0618\n",
      "Epoch 5/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2105 - accuracy: 0.9165 - mean_squared_error: 0.0617 - val_loss: 0.2053 - val_accuracy: 0.9203 - val_mean_squared_error: 0.0592\n",
      "Epoch 6/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2022 - accuracy: 0.9197 - mean_squared_error: 0.0591 - val_loss: 0.1997 - val_accuracy: 0.9225 - val_mean_squared_error: 0.0573\n",
      "Epoch 7/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1951 - accuracy: 0.9227 - mean_squared_error: 0.0568 - val_loss: 0.1933 - val_accuracy: 0.9259 - val_mean_squared_error: 0.0552\n",
      "Epoch 8/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1890 - accuracy: 0.9256 - mean_squared_error: 0.0549 - val_loss: 0.1887 - val_accuracy: 0.9276 - val_mean_squared_error: 0.0541\n",
      "Epoch 9/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9285 - mean_squared_error: 0.0533 - val_loss: 0.1855 - val_accuracy: 0.9315 - val_mean_squared_error: 0.0524\n",
      "Epoch 10/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1803 - accuracy: 0.9310 - mean_squared_error: 0.0520 - val_loss: 0.1822 - val_accuracy: 0.9320 - val_mean_squared_error: 0.0514\n",
      "Epoch 11/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1768 - accuracy: 0.9322 - mean_squared_error: 0.0510 - val_loss: 0.1799 - val_accuracy: 0.9337 - val_mean_squared_error: 0.0507\n",
      "Epoch 12/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1742 - accuracy: 0.9329 - mean_squared_error: 0.0501 - val_loss: 0.1772 - val_accuracy: 0.9347 - val_mean_squared_error: 0.0499\n",
      "Epoch 13/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1717 - accuracy: 0.9345 - mean_squared_error: 0.0494 - val_loss: 0.1773 - val_accuracy: 0.9325 - val_mean_squared_error: 0.0497\n",
      "Epoch 14/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1696 - accuracy: 0.9343 - mean_squared_error: 0.0490 - val_loss: 0.1758 - val_accuracy: 0.9311 - val_mean_squared_error: 0.0504\n",
      "Epoch 15/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1678 - accuracy: 0.9361 - mean_squared_error: 0.0483 - val_loss: 0.1725 - val_accuracy: 0.9369 - val_mean_squared_error: 0.0488\n",
      "Epoch 16/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1664 - accuracy: 0.9365 - mean_squared_error: 0.0480 - val_loss: 0.1710 - val_accuracy: 0.9357 - val_mean_squared_error: 0.0484\n",
      "Epoch 17/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1651 - accuracy: 0.9372 - mean_squared_error: 0.0476 - val_loss: 0.1691 - val_accuracy: 0.9373 - val_mean_squared_error: 0.0477\n",
      "Epoch 18/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1638 - accuracy: 0.9371 - mean_squared_error: 0.0472 - val_loss: 0.1699 - val_accuracy: 0.9371 - val_mean_squared_error: 0.0476\n",
      "Epoch 19/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1628 - accuracy: 0.9376 - mean_squared_error: 0.0470 - val_loss: 0.1678 - val_accuracy: 0.9369 - val_mean_squared_error: 0.0477\n",
      "Epoch 20/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1618 - accuracy: 0.9387 - mean_squared_error: 0.0466 - val_loss: 0.1674 - val_accuracy: 0.9369 - val_mean_squared_error: 0.0473\n",
      "Epoch 21/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1610 - accuracy: 0.9376 - mean_squared_error: 0.0465 - val_loss: 0.1672 - val_accuracy: 0.9356 - val_mean_squared_error: 0.0477\n",
      "Epoch 22/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1605 - accuracy: 0.9388 - mean_squared_error: 0.0463 - val_loss: 0.1656 - val_accuracy: 0.9371 - val_mean_squared_error: 0.0470\n",
      "Epoch 23/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1595 - accuracy: 0.9389 - mean_squared_error: 0.0461 - val_loss: 0.1659 - val_accuracy: 0.9383 - val_mean_squared_error: 0.0469\n",
      "Epoch 24/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1590 - accuracy: 0.9383 - mean_squared_error: 0.0459 - val_loss: 0.1656 - val_accuracy: 0.9391 - val_mean_squared_error: 0.0468\n",
      "Epoch 25/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1584 - accuracy: 0.9392 - mean_squared_error: 0.0457 - val_loss: 0.1636 - val_accuracy: 0.9389 - val_mean_squared_error: 0.0463\n",
      "Epoch 26/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1577 - accuracy: 0.9397 - mean_squared_error: 0.0455 - val_loss: 0.1650 - val_accuracy: 0.9387 - val_mean_squared_error: 0.0464\n",
      "Epoch 27/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1572 - accuracy: 0.9401 - mean_squared_error: 0.0453 - val_loss: 0.1633 - val_accuracy: 0.9381 - val_mean_squared_error: 0.0466\n",
      "Epoch 28/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1565 - accuracy: 0.9400 - mean_squared_error: 0.0451 - val_loss: 0.1632 - val_accuracy: 0.9392 - val_mean_squared_error: 0.0460\n",
      "Epoch 29/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1557 - accuracy: 0.9402 - mean_squared_error: 0.0450 - val_loss: 0.1626 - val_accuracy: 0.9391 - val_mean_squared_error: 0.0459\n",
      "Epoch 30/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1555 - accuracy: 0.9408 - mean_squared_error: 0.0449 - val_loss: 0.1608 - val_accuracy: 0.9413 - val_mean_squared_error: 0.0454\n",
      "Epoch 31/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1546 - accuracy: 0.9410 - mean_squared_error: 0.0446 - val_loss: 0.1621 - val_accuracy: 0.9393 - val_mean_squared_error: 0.0460\n",
      "Epoch 32/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1542 - accuracy: 0.9407 - mean_squared_error: 0.0445 - val_loss: 0.1609 - val_accuracy: 0.9379 - val_mean_squared_error: 0.0458\n",
      "Epoch 33/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1531 - accuracy: 0.9415 - mean_squared_error: 0.0441 - val_loss: 0.1590 - val_accuracy: 0.9400 - val_mean_squared_error: 0.0451\n",
      "Epoch 34/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1524 - accuracy: 0.9422 - mean_squared_error: 0.0439 - val_loss: 0.1581 - val_accuracy: 0.9428 - val_mean_squared_error: 0.0444\n",
      "Epoch 35/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1518 - accuracy: 0.9423 - mean_squared_error: 0.0436 - val_loss: 0.1597 - val_accuracy: 0.9411 - val_mean_squared_error: 0.0448\n",
      "Epoch 36/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1511 - accuracy: 0.9424 - mean_squared_error: 0.0434 - val_loss: 0.1584 - val_accuracy: 0.9424 - val_mean_squared_error: 0.0445\n",
      "Epoch 37/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1509 - accuracy: 0.9424 - mean_squared_error: 0.0435 - val_loss: 0.1568 - val_accuracy: 0.9435 - val_mean_squared_error: 0.0439\n",
      "Epoch 38/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1500 - accuracy: 0.9427 - mean_squared_error: 0.0431 - val_loss: 0.1571 - val_accuracy: 0.9404 - val_mean_squared_error: 0.0443\n",
      "Epoch 39/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1494 - accuracy: 0.9430 - mean_squared_error: 0.0429 - val_loss: 0.1560 - val_accuracy: 0.9436 - val_mean_squared_error: 0.0436\n",
      "Epoch 40/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1491 - accuracy: 0.9432 - mean_squared_error: 0.0429 - val_loss: 0.1576 - val_accuracy: 0.9400 - val_mean_squared_error: 0.0446\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1485 - accuracy: 0.9433 - mean_squared_error: 0.0427 - val_loss: 0.1548 - val_accuracy: 0.9432 - val_mean_squared_error: 0.0434\n",
      "Epoch 42/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1481 - accuracy: 0.9442 - mean_squared_error: 0.0425 - val_loss: 0.1542 - val_accuracy: 0.9429 - val_mean_squared_error: 0.0433\n",
      "Epoch 43/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1473 - accuracy: 0.9440 - mean_squared_error: 0.0422 - val_loss: 0.1569 - val_accuracy: 0.9431 - val_mean_squared_error: 0.0441\n",
      "Epoch 44/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1474 - accuracy: 0.9440 - mean_squared_error: 0.0423 - val_loss: 0.1538 - val_accuracy: 0.9443 - val_mean_squared_error: 0.0431\n",
      "Epoch 45/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1467 - accuracy: 0.9447 - mean_squared_error: 0.0420 - val_loss: 0.1533 - val_accuracy: 0.9453 - val_mean_squared_error: 0.0430\n",
      "Epoch 46/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1463 - accuracy: 0.9439 - mean_squared_error: 0.0420 - val_loss: 0.1539 - val_accuracy: 0.9439 - val_mean_squared_error: 0.0432\n",
      "Epoch 47/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1465 - accuracy: 0.9449 - mean_squared_error: 0.0420 - val_loss: 0.1537 - val_accuracy: 0.9440 - val_mean_squared_error: 0.0428\n",
      "Epoch 48/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1458 - accuracy: 0.9445 - mean_squared_error: 0.0418 - val_loss: 0.1535 - val_accuracy: 0.9437 - val_mean_squared_error: 0.0433\n",
      "Epoch 49/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1456 - accuracy: 0.9446 - mean_squared_error: 0.0418 - val_loss: 0.1524 - val_accuracy: 0.9441 - val_mean_squared_error: 0.0428\n",
      "Epoch 50/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1454 - accuracy: 0.9449 - mean_squared_error: 0.0417 - val_loss: 0.1536 - val_accuracy: 0.9449 - val_mean_squared_error: 0.0431\n",
      "\n",
      "accuracy: 0.9454850554466248 | mean_squared_error: 0.042147450149059296\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(10, activation='tanh'),\n",
    "    # output layer\n",
    "    Dense(2, activation='sigmoid'),\n",
    "])\n",
    "train_and_evaluate_model(model, data, loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we observe an increase in the performance of the model. The next parameter to modify will be the momentum. The momentum can be interpreted as the inertia kept by the weight updates. So when it's time to update a weight using the obtained gradient, it will also consider the previous gradients obtained for that weight. Each of the previous gradients will be weighted in an exponential decay determined by the value of the momentum parameter. This will help for example to avoid getting stuck in local minima. For this, I tried adding momentum=0.9 as a new parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "469/469 [==============================] - 2s 2ms/step - loss: 0.2394 - accuracy: 0.9017 - mean_squared_error: 0.0718 - val_loss: 0.1945 - val_accuracy: 0.9208 - val_mean_squared_error: 0.0567\n",
      "Epoch 2/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1836 - accuracy: 0.9277 - mean_squared_error: 0.0537 - val_loss: 0.1812 - val_accuracy: 0.9315 - val_mean_squared_error: 0.0514\n",
      "Epoch 3/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1687 - accuracy: 0.9350 - mean_squared_error: 0.0491 - val_loss: 0.1641 - val_accuracy: 0.9388 - val_mean_squared_error: 0.0465\n",
      "Epoch 4/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1634 - accuracy: 0.9372 - mean_squared_error: 0.0474 - val_loss: 0.1603 - val_accuracy: 0.9412 - val_mean_squared_error: 0.0450\n",
      "Epoch 5/50\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.1588 - accuracy: 0.9382 - mean_squared_error: 0.0461 - val_loss: 0.1618 - val_accuracy: 0.9381 - val_mean_squared_error: 0.0458\n",
      "Epoch 6/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1570 - accuracy: 0.9385 - mean_squared_error: 0.0458 - val_loss: 0.1593 - val_accuracy: 0.9432 - val_mean_squared_error: 0.0448\n",
      "Epoch 7/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1557 - accuracy: 0.9397 - mean_squared_error: 0.0452 - val_loss: 0.1585 - val_accuracy: 0.9416 - val_mean_squared_error: 0.0453\n",
      "Epoch 8/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1526 - accuracy: 0.9419 - mean_squared_error: 0.0442 - val_loss: 0.1540 - val_accuracy: 0.9415 - val_mean_squared_error: 0.0439\n",
      "Epoch 9/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1531 - accuracy: 0.9404 - mean_squared_error: 0.0445 - val_loss: 0.1511 - val_accuracy: 0.9431 - val_mean_squared_error: 0.0430\n",
      "Epoch 10/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1506 - accuracy: 0.9419 - mean_squared_error: 0.0438 - val_loss: 0.1581 - val_accuracy: 0.9404 - val_mean_squared_error: 0.0454\n",
      "Epoch 11/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1491 - accuracy: 0.9432 - mean_squared_error: 0.0430 - val_loss: 0.1563 - val_accuracy: 0.9401 - val_mean_squared_error: 0.0447\n",
      "Epoch 12/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1500 - accuracy: 0.9419 - mean_squared_error: 0.0434 - val_loss: 0.1596 - val_accuracy: 0.9407 - val_mean_squared_error: 0.0452\n",
      "Epoch 13/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1488 - accuracy: 0.9430 - mean_squared_error: 0.0430 - val_loss: 0.1600 - val_accuracy: 0.9389 - val_mean_squared_error: 0.0460\n",
      "Epoch 14/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1475 - accuracy: 0.9430 - mean_squared_error: 0.0427 - val_loss: 0.1696 - val_accuracy: 0.9359 - val_mean_squared_error: 0.0489\n",
      "Epoch 15/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1481 - accuracy: 0.9427 - mean_squared_error: 0.0429 - val_loss: 0.1539 - val_accuracy: 0.9416 - val_mean_squared_error: 0.0439\n",
      "Epoch 16/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1493 - accuracy: 0.9434 - mean_squared_error: 0.0432 - val_loss: 0.1507 - val_accuracy: 0.9449 - val_mean_squared_error: 0.0427\n",
      "Epoch 17/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1485 - accuracy: 0.9424 - mean_squared_error: 0.0430 - val_loss: 0.1552 - val_accuracy: 0.9400 - val_mean_squared_error: 0.0445\n",
      "Epoch 18/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1487 - accuracy: 0.9424 - mean_squared_error: 0.0429 - val_loss: 0.1496 - val_accuracy: 0.9465 - val_mean_squared_error: 0.0417\n",
      "Epoch 19/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1478 - accuracy: 0.9425 - mean_squared_error: 0.0428 - val_loss: 0.1506 - val_accuracy: 0.9445 - val_mean_squared_error: 0.0426\n",
      "Epoch 20/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1465 - accuracy: 0.9441 - mean_squared_error: 0.0424 - val_loss: 0.1536 - val_accuracy: 0.9441 - val_mean_squared_error: 0.0430\n",
      "Epoch 21/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1460 - accuracy: 0.9430 - mean_squared_error: 0.0424 - val_loss: 0.1483 - val_accuracy: 0.9448 - val_mean_squared_error: 0.0418\n",
      "Epoch 22/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1456 - accuracy: 0.9424 - mean_squared_error: 0.0422 - val_loss: 0.1535 - val_accuracy: 0.9415 - val_mean_squared_error: 0.0437\n",
      "Epoch 23/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1466 - accuracy: 0.9430 - mean_squared_error: 0.0426 - val_loss: 0.1530 - val_accuracy: 0.9443 - val_mean_squared_error: 0.0432\n",
      "Epoch 24/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1459 - accuracy: 0.9424 - mean_squared_error: 0.0424 - val_loss: 0.1576 - val_accuracy: 0.9423 - val_mean_squared_error: 0.0443\n",
      "Epoch 25/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1465 - accuracy: 0.9435 - mean_squared_error: 0.0423 - val_loss: 0.1500 - val_accuracy: 0.9455 - val_mean_squared_error: 0.0422\n",
      "Epoch 26/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1450 - accuracy: 0.9435 - mean_squared_error: 0.0421 - val_loss: 0.1532 - val_accuracy: 0.9411 - val_mean_squared_error: 0.0436\n",
      "Epoch 27/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1431 - accuracy: 0.9440 - mean_squared_error: 0.0416 - val_loss: 0.1453 - val_accuracy: 0.9464 - val_mean_squared_error: 0.0411\n",
      "Epoch 28/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1413 - accuracy: 0.9460 - mean_squared_error: 0.0409 - val_loss: 0.1463 - val_accuracy: 0.9469 - val_mean_squared_error: 0.0409\n",
      "Epoch 29/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1403 - accuracy: 0.9465 - mean_squared_error: 0.0405 - val_loss: 0.1456 - val_accuracy: 0.9473 - val_mean_squared_error: 0.0406\n",
      "Epoch 30/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1375 - accuracy: 0.9474 - mean_squared_error: 0.0397 - val_loss: 0.1439 - val_accuracy: 0.9445 - val_mean_squared_error: 0.0410\n",
      "Epoch 31/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1383 - accuracy: 0.9467 - mean_squared_error: 0.0399 - val_loss: 0.1566 - val_accuracy: 0.9441 - val_mean_squared_error: 0.0437\n",
      "Epoch 32/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1373 - accuracy: 0.9473 - mean_squared_error: 0.0396 - val_loss: 0.1455 - val_accuracy: 0.9448 - val_mean_squared_error: 0.0414\n",
      "Epoch 33/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1359 - accuracy: 0.9478 - mean_squared_error: 0.0391 - val_loss: 0.1416 - val_accuracy: 0.9471 - val_mean_squared_error: 0.0400\n",
      "Epoch 34/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1364 - accuracy: 0.9490 - mean_squared_error: 0.0393 - val_loss: 0.1463 - val_accuracy: 0.9448 - val_mean_squared_error: 0.0412\n",
      "Epoch 35/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1339 - accuracy: 0.9488 - mean_squared_error: 0.0386 - val_loss: 0.1442 - val_accuracy: 0.9484 - val_mean_squared_error: 0.0401\n",
      "Epoch 36/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1351 - accuracy: 0.9486 - mean_squared_error: 0.0386 - val_loss: 0.1418 - val_accuracy: 0.9503 - val_mean_squared_error: 0.0393\n",
      "Epoch 37/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1349 - accuracy: 0.9478 - mean_squared_error: 0.0391 - val_loss: 0.1473 - val_accuracy: 0.9464 - val_mean_squared_error: 0.0410\n",
      "Epoch 38/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1348 - accuracy: 0.9493 - mean_squared_error: 0.0387 - val_loss: 0.1390 - val_accuracy: 0.9487 - val_mean_squared_error: 0.0388\n",
      "Epoch 39/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1344 - accuracy: 0.9480 - mean_squared_error: 0.0390 - val_loss: 0.1402 - val_accuracy: 0.9501 - val_mean_squared_error: 0.0387\n",
      "Epoch 40/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1341 - accuracy: 0.9483 - mean_squared_error: 0.0385 - val_loss: 0.1443 - val_accuracy: 0.9455 - val_mean_squared_error: 0.0405\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1340 - accuracy: 0.9490 - mean_squared_error: 0.0385 - val_loss: 0.1379 - val_accuracy: 0.9504 - val_mean_squared_error: 0.0379\n",
      "Epoch 42/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1334 - accuracy: 0.9491 - mean_squared_error: 0.0384 - val_loss: 0.1386 - val_accuracy: 0.9489 - val_mean_squared_error: 0.0387\n",
      "Epoch 43/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1326 - accuracy: 0.9497 - mean_squared_error: 0.0381 - val_loss: 0.1446 - val_accuracy: 0.9481 - val_mean_squared_error: 0.0403\n",
      "Epoch 44/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1334 - accuracy: 0.9489 - mean_squared_error: 0.0382 - val_loss: 0.1385 - val_accuracy: 0.9492 - val_mean_squared_error: 0.0385\n",
      "Epoch 45/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1323 - accuracy: 0.9498 - mean_squared_error: 0.0379 - val_loss: 0.1373 - val_accuracy: 0.9521 - val_mean_squared_error: 0.0379\n",
      "Epoch 46/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1330 - accuracy: 0.9490 - mean_squared_error: 0.0383 - val_loss: 0.1339 - val_accuracy: 0.9516 - val_mean_squared_error: 0.0373\n",
      "Epoch 47/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1334 - accuracy: 0.9492 - mean_squared_error: 0.0383 - val_loss: 0.1363 - val_accuracy: 0.9509 - val_mean_squared_error: 0.0376\n",
      "Epoch 48/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1329 - accuracy: 0.9495 - mean_squared_error: 0.0382 - val_loss: 0.1373 - val_accuracy: 0.9499 - val_mean_squared_error: 0.0380\n",
      "Epoch 49/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1318 - accuracy: 0.9491 - mean_squared_error: 0.0379 - val_loss: 0.1385 - val_accuracy: 0.9481 - val_mean_squared_error: 0.0387\n",
      "Epoch 50/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1312 - accuracy: 0.9501 - mean_squared_error: 0.0378 - val_loss: 0.1400 - val_accuracy: 0.9495 - val_mean_squared_error: 0.0388\n",
      "\n",
      "accuracy: 0.9494911432266235 | mean_squared_error: 0.03882622346282005\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(10, activation='tanh'),\n",
    "    # output layer\n",
    "    Dense(2, activation='sigmoid'),\n",
    "])\n",
    "train_and_evaluate_model(model, data, loss=\"binary_crossentropy\", momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the metrics improve again. I will try now to add a second hidden layer with tanh as the activation function. Note that adding more hidden layers increases the complexity of the model, which can sometimes lead to poorer performances. The new network should look as the following diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" style=\"cursor: move;\" width=\"1872\" height=\"939\"><g transform=\"translate(-453.4407967506179,-189.33900437484783) scale(1.3717332885905962)\"><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,279.5, 902.6666666666666,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,319.5, 902.6666666666666,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,359.5, 902.6666666666666,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,399.5, 902.6666666666666,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,439.5, 902.6666666666666,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,479.5, 902.6666666666666,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,519.5, 902.6666666666666,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,559.5, 902.6666666666666,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,599.5, 902.6666666666666,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,639.5, 902.6666666666666,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,279.5, 1082.6666666666667,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,279.5, 902.6666666666666,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,319.5, 902.6666666666666,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,359.5, 902.6666666666666,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,399.5, 902.6666666666666,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,439.5, 902.6666666666666,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,479.5, 902.6666666666666,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,519.5, 902.6666666666666,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,559.5, 902.6666666666666,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,599.5, 902.6666666666666,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,639.5, 902.6666666666666,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,319.5, 1082.6666666666667,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,279.5, 902.6666666666666,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,319.5, 902.6666666666666,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,359.5, 902.6666666666666,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,399.5, 902.6666666666666,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,439.5, 902.6666666666666,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,479.5, 902.6666666666666,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,519.5, 902.6666666666666,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,559.5, 902.6666666666666,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,599.5, 902.6666666666666,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,639.5, 902.6666666666666,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,359.5, 1082.6666666666667,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,279.5, 902.6666666666666,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,319.5, 902.6666666666666,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,359.5, 902.6666666666666,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,399.5, 902.6666666666666,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,439.5, 902.6666666666666,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,479.5, 902.6666666666666,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,519.5, 902.6666666666666,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,559.5, 902.6666666666666,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,599.5, 902.6666666666666,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,639.5, 902.6666666666666,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,399.5, 1082.6666666666667,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,279.5, 902.6666666666666,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,319.5, 902.6666666666666,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,359.5, 902.6666666666666,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,399.5, 902.6666666666666,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,439.5, 902.6666666666666,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,479.5, 902.6666666666666,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,519.5, 902.6666666666666,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,559.5, 902.6666666666666,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,599.5, 902.6666666666666,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,639.5, 902.6666666666666,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,439.5, 1082.6666666666667,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,279.5, 902.6666666666666,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,319.5, 902.6666666666666,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,359.5, 902.6666666666666,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,399.5, 902.6666666666666,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,439.5, 902.6666666666666,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,479.5, 902.6666666666666,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,519.5, 902.6666666666666,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,559.5, 902.6666666666666,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,599.5, 902.6666666666666,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,639.5, 902.6666666666666,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,479.5, 1082.6666666666667,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,279.5, 902.6666666666666,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,319.5, 902.6666666666666,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,359.5, 902.6666666666666,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,399.5, 902.6666666666666,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,439.5, 902.6666666666666,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,479.5, 902.6666666666666,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,519.5, 902.6666666666666,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,559.5, 902.6666666666666,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,599.5, 902.6666666666666,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,639.5, 902.6666666666666,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,519.5, 1082.6666666666667,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,279.5, 902.6666666666666,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,319.5, 902.6666666666666,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,359.5, 902.6666666666666,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,399.5, 902.6666666666666,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,439.5, 902.6666666666666,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,479.5, 902.6666666666666,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,519.5, 902.6666666666666,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,559.5, 902.6666666666666,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,599.5, 902.6666666666666,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,639.5, 902.6666666666666,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,559.5, 1082.6666666666667,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,279.5, 902.6666666666666,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,319.5, 902.6666666666666,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,359.5, 902.6666666666666,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,399.5, 902.6666666666666,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,439.5, 902.6666666666666,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,479.5, 902.6666666666666,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,519.5, 902.6666666666666,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,559.5, 902.6666666666666,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,599.5, 902.6666666666666,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,639.5, 902.6666666666666,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,599.5, 1082.6666666666667,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,279.5, 902.6666666666666,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,319.5, 902.6666666666666,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,359.5, 902.6666666666666,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,399.5, 902.6666666666666,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,439.5, 902.6666666666666,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,479.5, 902.6666666666666,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,519.5, 902.6666666666666,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,559.5, 902.6666666666666,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,599.5, 902.6666666666666,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M722.6666666666666,639.5, 902.6666666666666,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,639.5, 1082.6666666666667,279.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,279.5, 1262.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,279.5, 1082.6666666666667,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,319.5, 1082.6666666666667,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,359.5, 1082.6666666666667,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,399.5, 1082.6666666666667,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,439.5, 1082.6666666666667,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,479.5, 1082.6666666666667,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,519.5, 1082.6666666666667,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,559.5, 1082.6666666666667,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,599.5, 1082.6666666666667,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,639.5, 1082.6666666666667,319.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,319.5, 1262.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,279.5, 1082.6666666666667,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,319.5, 1082.6666666666667,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,359.5, 1082.6666666666667,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,399.5, 1082.6666666666667,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,439.5, 1082.6666666666667,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,479.5, 1082.6666666666667,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,519.5, 1082.6666666666667,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,559.5, 1082.6666666666667,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,599.5, 1082.6666666666667,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,639.5, 1082.6666666666667,359.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,359.5, 1262.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,279.5, 1082.6666666666667,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,319.5, 1082.6666666666667,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,359.5, 1082.6666666666667,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,399.5, 1082.6666666666667,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,439.5, 1082.6666666666667,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,479.5, 1082.6666666666667,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,519.5, 1082.6666666666667,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,559.5, 1082.6666666666667,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,599.5, 1082.6666666666667,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,639.5, 1082.6666666666667,399.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,399.5, 1262.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,279.5, 1082.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,319.5, 1082.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,359.5, 1082.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,399.5, 1082.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,439.5, 1082.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,479.5, 1082.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,519.5, 1082.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,559.5, 1082.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,599.5, 1082.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,639.5, 1082.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,439.5, 1262.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,279.5, 1082.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,319.5, 1082.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,359.5, 1082.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,399.5, 1082.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,439.5, 1082.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,479.5, 1082.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,519.5, 1082.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,559.5, 1082.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,599.5, 1082.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,639.5, 1082.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,479.5, 1262.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,279.5, 1082.6666666666667,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,319.5, 1082.6666666666667,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,359.5, 1082.6666666666667,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,399.5, 1082.6666666666667,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,439.5, 1082.6666666666667,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,479.5, 1082.6666666666667,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,519.5, 1082.6666666666667,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,559.5, 1082.6666666666667,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,599.5, 1082.6666666666667,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,639.5, 1082.6666666666667,519.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,519.5, 1262.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,279.5, 1082.6666666666667,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,319.5, 1082.6666666666667,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,359.5, 1082.6666666666667,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,399.5, 1082.6666666666667,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,439.5, 1082.6666666666667,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,479.5, 1082.6666666666667,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,519.5, 1082.6666666666667,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,559.5, 1082.6666666666667,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,599.5, 1082.6666666666667,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,639.5, 1082.6666666666667,559.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,559.5, 1262.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,279.5, 1082.6666666666667,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,319.5, 1082.6666666666667,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,359.5, 1082.6666666666667,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,399.5, 1082.6666666666667,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,439.5, 1082.6666666666667,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,479.5, 1082.6666666666667,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,519.5, 1082.6666666666667,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,559.5, 1082.6666666666667,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,599.5, 1082.6666666666667,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,639.5, 1082.6666666666667,599.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,599.5, 1262.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,279.5, 1082.6666666666667,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,319.5, 1082.6666666666667,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,359.5, 1082.6666666666667,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,399.5, 1082.6666666666667,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,439.5, 1082.6666666666667,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,479.5, 1082.6666666666667,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,519.5, 1082.6666666666667,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,559.5, 1082.6666666666667,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,599.5, 1082.6666666666667,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M902.6666666666666,639.5, 1082.6666666666667,639.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,639.5, 1262.6666666666667,439.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,279.5, 1262.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,319.5, 1262.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,359.5, 1262.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,399.5, 1262.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,439.5, 1262.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,479.5, 1262.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,519.5, 1262.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,559.5, 1262.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,599.5, 1262.6666666666667,479.5\"/><path class=\"link\" style=\"stroke-width: 0.5px; stroke-opacity: 1; stroke: rgb(80, 80, 80);\" marker-end=\"\" d=\"M1082.6666666666667,639.5, 1262.6666666666667,479.5\"/><circle r=\"10\" class=\"node\" id=\"0_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"722.6666666666666\" cy=\"279.5\"/><circle r=\"10\" class=\"node\" id=\"0_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"722.6666666666666\" cy=\"319.5\"/><circle r=\"10\" class=\"node\" id=\"0_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"722.6666666666666\" cy=\"359.5\"/><circle r=\"10\" class=\"node\" id=\"0_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"722.6666666666666\" cy=\"399.5\"/><circle r=\"10\" class=\"node\" id=\"0_4\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"722.6666666666666\" cy=\"439.5\"/><circle r=\"10\" class=\"node\" id=\"0_5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"722.6666666666666\" cy=\"479.5\"/><circle r=\"10\" class=\"node\" id=\"0_6\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"722.6666666666666\" cy=\"519.5\"/><circle r=\"10\" class=\"node\" id=\"0_7\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"722.6666666666666\" cy=\"559.5\"/><circle r=\"10\" class=\"node\" id=\"0_8\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"722.6666666666666\" cy=\"599.5\"/><circle r=\"10\" class=\"node\" id=\"0_9\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"722.6666666666666\" cy=\"639.5\"/><circle r=\"10\" class=\"node\" id=\"1_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"902.6666666666666\" cy=\"279.5\"/><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"687.6666666666666\" y=\"679.5\">Input Layer ∈ ℝ¹⁰</text><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"867.6666666666666\" y=\"679.5\">Hidden Layer ∈ ℝ¹⁰</text><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"1047.6666666666667\" y=\"679.5\">Hidden Layer ∈ ℝ¹⁰</text><circle r=\"10\" class=\"node\" id=\"1_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"902.6666666666666\" cy=\"319.5\"/><circle r=\"10\" class=\"node\" id=\"1_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"902.6666666666666\" cy=\"359.5\"/><circle r=\"10\" class=\"node\" id=\"1_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"902.6666666666666\" cy=\"399.5\"/><circle r=\"10\" class=\"node\" id=\"1_4\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"902.6666666666666\" cy=\"439.5\"/><circle r=\"10\" class=\"node\" id=\"1_5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"902.6666666666666\" cy=\"479.5\"/><circle r=\"10\" class=\"node\" id=\"1_6\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"902.6666666666666\" cy=\"519.5\"/><circle r=\"10\" class=\"node\" id=\"1_7\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"902.6666666666666\" cy=\"559.5\"/><circle r=\"10\" class=\"node\" id=\"1_8\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"902.6666666666666\" cy=\"599.5\"/><circle r=\"10\" class=\"node\" id=\"1_9\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"902.6666666666666\" cy=\"639.5\"/><circle r=\"10\" class=\"node\" id=\"2_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1082.6666666666667\" cy=\"279.5\"/><circle r=\"10\" class=\"node\" id=\"2_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1082.6666666666667\" cy=\"319.5\"/><circle r=\"10\" class=\"node\" id=\"2_2\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1082.6666666666667\" cy=\"359.5\"/><circle r=\"10\" class=\"node\" id=\"2_3\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1082.6666666666667\" cy=\"399.5\"/><circle r=\"10\" class=\"node\" id=\"2_4\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1082.6666666666667\" cy=\"439.5\"/><circle r=\"10\" class=\"node\" id=\"2_5\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1082.6666666666667\" cy=\"479.5\"/><circle r=\"10\" class=\"node\" id=\"2_6\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1082.6666666666667\" cy=\"519.5\"/><circle r=\"10\" class=\"node\" id=\"2_7\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1082.6666666666667\" cy=\"559.5\"/><circle r=\"10\" class=\"node\" id=\"2_8\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1082.6666666666667\" cy=\"599.5\"/><circle r=\"10\" class=\"node\" id=\"2_9\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1082.6666666666667\" cy=\"639.5\"/><circle r=\"10\" class=\"node\" id=\"3_0\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1262.6666666666667\" cy=\"439.5\"/><text class=\"text\" dy=\".35em\" style=\"font-size: 12px;\" x=\"1227.6666666666667\" y=\"679.5\">Output Layer ∈ ℝ²</text><circle r=\"10\" class=\"node\" id=\"3_1\" style=\"fill: rgb(255, 255, 255); stroke: rgb(51, 51, 51);\" cx=\"1262.6666666666667\" cy=\"479.5\"/></g><defs><marker id=\"arrow\" viewBox=\"0 -5 10 10\" markerWidth=\"7\" markerHeight=\"7\" orient=\"auto\" refX=\"40\"><path d=\"M0,-5L10,0L0,5\" style=\"stroke: rgb(80, 80, 80); fill: none;\"/></marker></defs></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import SVG\n",
    "SVG(filename='assets/nn-2-hidden.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2263 - accuracy: 0.9075 - mean_squared_error: 0.0675 - val_loss: 0.1913 - val_accuracy: 0.9243 - val_mean_squared_error: 0.0557\n",
      "Epoch 2/50\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.9303 - mean_squared_error: 0.0519 - val_loss: 0.1795 - val_accuracy: 0.9312 - val_mean_squared_error: 0.0516\n",
      "Epoch 3/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1675 - accuracy: 0.9352 - mean_squared_error: 0.0488 - val_loss: 0.1579 - val_accuracy: 0.9411 - val_mean_squared_error: 0.0449\n",
      "Epoch 4/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1596 - accuracy: 0.9393 - mean_squared_error: 0.0461 - val_loss: 0.1527 - val_accuracy: 0.9393 - val_mean_squared_error: 0.0440\n",
      "Epoch 5/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1567 - accuracy: 0.9396 - mean_squared_error: 0.0454 - val_loss: 0.1534 - val_accuracy: 0.9413 - val_mean_squared_error: 0.0438\n",
      "Epoch 6/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1542 - accuracy: 0.9406 - mean_squared_error: 0.0447 - val_loss: 0.1540 - val_accuracy: 0.9429 - val_mean_squared_error: 0.0436\n",
      "Epoch 7/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1511 - accuracy: 0.9422 - mean_squared_error: 0.0437 - val_loss: 0.1544 - val_accuracy: 0.9415 - val_mean_squared_error: 0.0442\n",
      "Epoch 8/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1484 - accuracy: 0.9429 - mean_squared_error: 0.0432 - val_loss: 0.1539 - val_accuracy: 0.9419 - val_mean_squared_error: 0.0438\n",
      "Epoch 9/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1487 - accuracy: 0.9431 - mean_squared_error: 0.0430 - val_loss: 0.1432 - val_accuracy: 0.9463 - val_mean_squared_error: 0.0405\n",
      "Epoch 10/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1467 - accuracy: 0.9451 - mean_squared_error: 0.0422 - val_loss: 0.1538 - val_accuracy: 0.9459 - val_mean_squared_error: 0.0437\n",
      "Epoch 11/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1447 - accuracy: 0.9458 - mean_squared_error: 0.0416 - val_loss: 0.1533 - val_accuracy: 0.9404 - val_mean_squared_error: 0.0444\n",
      "Epoch 12/50\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1455 - accuracy: 0.9437 - mean_squared_error: 0.0422 - val_loss: 0.1517 - val_accuracy: 0.9436 - val_mean_squared_error: 0.0430\n",
      "Epoch 13/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1437 - accuracy: 0.9446 - mean_squared_error: 0.0416 - val_loss: 0.1558 - val_accuracy: 0.9389 - val_mean_squared_error: 0.0451\n",
      "Epoch 14/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1429 - accuracy: 0.9458 - mean_squared_error: 0.0413 - val_loss: 0.1666 - val_accuracy: 0.9383 - val_mean_squared_error: 0.0475\n",
      "Epoch 15/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1422 - accuracy: 0.9468 - mean_squared_error: 0.0410 - val_loss: 0.1476 - val_accuracy: 0.9431 - val_mean_squared_error: 0.0423\n",
      "Epoch 16/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1439 - accuracy: 0.9450 - mean_squared_error: 0.0417 - val_loss: 0.1451 - val_accuracy: 0.9472 - val_mean_squared_error: 0.0411\n",
      "Epoch 17/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1419 - accuracy: 0.9460 - mean_squared_error: 0.0412 - val_loss: 0.1503 - val_accuracy: 0.9427 - val_mean_squared_error: 0.0428\n",
      "Epoch 18/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1411 - accuracy: 0.9450 - mean_squared_error: 0.0408 - val_loss: 0.1432 - val_accuracy: 0.9472 - val_mean_squared_error: 0.0402\n",
      "Epoch 19/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1414 - accuracy: 0.9460 - mean_squared_error: 0.0410 - val_loss: 0.1423 - val_accuracy: 0.9437 - val_mean_squared_error: 0.0408\n",
      "Epoch 20/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1371 - accuracy: 0.9476 - mean_squared_error: 0.0396 - val_loss: 0.1415 - val_accuracy: 0.9472 - val_mean_squared_error: 0.0402\n",
      "Epoch 21/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1368 - accuracy: 0.9467 - mean_squared_error: 0.0397 - val_loss: 0.1390 - val_accuracy: 0.9475 - val_mean_squared_error: 0.0396\n",
      "Epoch 22/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1352 - accuracy: 0.9477 - mean_squared_error: 0.0393 - val_loss: 0.1417 - val_accuracy: 0.9453 - val_mean_squared_error: 0.0406\n",
      "Epoch 23/50\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1345 - accuracy: 0.9474 - mean_squared_error: 0.0389 - val_loss: 0.1423 - val_accuracy: 0.9484 - val_mean_squared_error: 0.0395\n",
      "Epoch 24/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1333 - accuracy: 0.9476 - mean_squared_error: 0.0388 - val_loss: 0.1435 - val_accuracy: 0.9471 - val_mean_squared_error: 0.0403\n",
      "Epoch 25/50\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1336 - accuracy: 0.9493 - mean_squared_error: 0.0384 - val_loss: 0.1459 - val_accuracy: 0.9443 - val_mean_squared_error: 0.0417\n",
      "Epoch 26/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1323 - accuracy: 0.9488 - mean_squared_error: 0.0384 - val_loss: 0.1495 - val_accuracy: 0.9432 - val_mean_squared_error: 0.0424\n",
      "Epoch 27/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1315 - accuracy: 0.9494 - mean_squared_error: 0.0380 - val_loss: 0.1401 - val_accuracy: 0.9471 - val_mean_squared_error: 0.0398\n",
      "Epoch 28/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1313 - accuracy: 0.9499 - mean_squared_error: 0.0378 - val_loss: 0.1418 - val_accuracy: 0.9484 - val_mean_squared_error: 0.0400\n",
      "Epoch 29/50\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1318 - accuracy: 0.9497 - mean_squared_error: 0.0381 - val_loss: 0.1385 - val_accuracy: 0.9491 - val_mean_squared_error: 0.0387\n",
      "Epoch 30/50\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1295 - accuracy: 0.9506 - mean_squared_error: 0.0371 - val_loss: 0.1393 - val_accuracy: 0.9459 - val_mean_squared_error: 0.0398\n",
      "Epoch 31/50\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1300 - accuracy: 0.9497 - mean_squared_error: 0.0377 - val_loss: 0.1403 - val_accuracy: 0.9475 - val_mean_squared_error: 0.0399\n",
      "Epoch 32/50\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1299 - accuracy: 0.9506 - mean_squared_error: 0.0374 - val_loss: 0.1510 - val_accuracy: 0.9443 - val_mean_squared_error: 0.0428\n",
      "Epoch 33/50\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1292 - accuracy: 0.9510 - mean_squared_error: 0.0370 - val_loss: 0.1326 - val_accuracy: 0.9500 - val_mean_squared_error: 0.0376\n",
      "Epoch 34/50\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1288 - accuracy: 0.9508 - mean_squared_error: 0.0370 - val_loss: 0.1381 - val_accuracy: 0.9488 - val_mean_squared_error: 0.0389\n",
      "Epoch 35/50\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1276 - accuracy: 0.9499 - mean_squared_error: 0.0370 - val_loss: 0.1366 - val_accuracy: 0.9477 - val_mean_squared_error: 0.0387\n",
      "Epoch 36/50\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1265 - accuracy: 0.9525 - mean_squared_error: 0.0363 - val_loss: 0.1377 - val_accuracy: 0.9496 - val_mean_squared_error: 0.0389\n",
      "Epoch 37/50\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1270 - accuracy: 0.9508 - mean_squared_error: 0.0369 - val_loss: 0.1493 - val_accuracy: 0.9425 - val_mean_squared_error: 0.0424\n",
      "Epoch 38/50\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1279 - accuracy: 0.9507 - mean_squared_error: 0.0368 - val_loss: 0.1349 - val_accuracy: 0.9496 - val_mean_squared_error: 0.0377\n",
      "Epoch 39/50\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1269 - accuracy: 0.9513 - mean_squared_error: 0.0367 - val_loss: 0.1355 - val_accuracy: 0.9491 - val_mean_squared_error: 0.0378\n",
      "Epoch 40/50\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1288 - accuracy: 0.9504 - mean_squared_error: 0.0371 - val_loss: 0.1481 - val_accuracy: 0.9444 - val_mean_squared_error: 0.0421\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 3s 5ms/step - loss: 0.1274 - accuracy: 0.9512 - mean_squared_error: 0.0366 - val_loss: 0.1397 - val_accuracy: 0.9480 - val_mean_squared_error: 0.0388\n",
      "Epoch 42/50\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.1264 - accuracy: 0.9510 - mean_squared_error: 0.0365 - val_loss: 0.1330 - val_accuracy: 0.9496 - val_mean_squared_error: 0.0379\n",
      "Epoch 43/50\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.1268 - accuracy: 0.9517 - mean_squared_error: 0.0364 - val_loss: 0.1426 - val_accuracy: 0.9447 - val_mean_squared_error: 0.0409\n",
      "Epoch 44/50\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.1271 - accuracy: 0.9514 - mean_squared_error: 0.0365 - val_loss: 0.1411 - val_accuracy: 0.9485 - val_mean_squared_error: 0.0400\n",
      "Epoch 45/50\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.1256 - accuracy: 0.9523 - mean_squared_error: 0.0359 - val_loss: 0.1294 - val_accuracy: 0.9512 - val_mean_squared_error: 0.0369\n",
      "Epoch 46/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1260 - accuracy: 0.9521 - mean_squared_error: 0.0363 - val_loss: 0.1310 - val_accuracy: 0.9496 - val_mean_squared_error: 0.0375\n",
      "Epoch 47/50\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.1259 - accuracy: 0.9509 - mean_squared_error: 0.0365 - val_loss: 0.1321 - val_accuracy: 0.9512 - val_mean_squared_error: 0.0368\n",
      "Epoch 48/50\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.1255 - accuracy: 0.9509 - mean_squared_error: 0.0361 - val_loss: 0.1357 - val_accuracy: 0.9496 - val_mean_squared_error: 0.0382\n",
      "Epoch 49/50\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.1245 - accuracy: 0.9522 - mean_squared_error: 0.0360 - val_loss: 0.1431 - val_accuracy: 0.9440 - val_mean_squared_error: 0.0407\n",
      "Epoch 50/50\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1252 - accuracy: 0.9522 - mean_squared_error: 0.0360 - val_loss: 0.1381 - val_accuracy: 0.9493 - val_mean_squared_error: 0.0386\n",
      "\n",
      "accuracy: 0.9511151909828186 | mean_squared_error: 0.03762579709291458\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(10, activation='tanh'),\n",
    "    Dense(10, activation='tanh'),\n",
    "    # output layer\n",
    "    Dense(2, activation='sigmoid'),\n",
    "])\n",
    "train_and_evaluate_model(model, data, loss=\"binary_crossentropy\", momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in this case, it does indeed improve the performance. So I will keep this structure for the rest of the exercise. Finally, what I want to do is to tune the last 2 parameters I have not yet touched. Those are the learning rate and the batch size. To do so, I will perform a grid search (try all the possible combinations of the given values) and keep the pair that produces the best evaluating metrics. The values I will try are the following ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [32, 64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy: 0.9444564580917358 | mean_squared_error: 0.04316599667072296\n",
      "\n",
      "accuracy: 0.9372022747993469 | mean_squared_error: 0.04739774391055107\n",
      "\n",
      "accuracy: 0.9280532598495483 | mean_squared_error: 0.05398264154791832\n",
      "\n",
      "accuracy: 0.9518189430236816 | mean_squared_error: 0.03721180558204651\n",
      "\n",
      "accuracy: 0.9495993852615356 | mean_squared_error: 0.038618169724941254\n",
      "\n",
      "accuracy: 0.9493287205696106 | mean_squared_error: 0.0392879918217659\n",
      "\n",
      "accuracy: 0.9495452642440796 | mean_squared_error: 0.038814522325992584\n",
      "\n",
      "accuracy: 0.9511151909828186 | mean_squared_error: 0.03762579709291458\n",
      "\n",
      "accuracy: 0.9472715258598328 | mean_squared_error: 0.039197757840156555\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "for lr, bs in itertools.product(learning_rates, batch_sizes):\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(10, activation='tanh'),\n",
    "        Dense(10, activation='tanh'),\n",
    "        # output layer\n",
    "        Dense(2, activation='sigmoid'),\n",
    "    ])\n",
    "    train_and_evaluate_model(model, data, loss=\"binary_crossentropy\", momentum=0.9, learning_rate=lr, batch_size=bs, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I train again the model using the final structure and the hyper-parameters I have just obtained (`learning_rate` = 0.01 and `batch_sizes` = 32). I will use the method train_model to obtain the trained model instead of just evaluating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "938/938 [==============================] - 5s 4ms/step - loss: 0.2596 - accuracy: 0.8955 - mean_squared_error: 0.0774 - val_loss: 0.2097 - val_accuracy: 0.9161 - val_mean_squared_error: 0.0611\n",
      "Epoch 2/50\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9219 - mean_squared_error: 0.0570 - val_loss: 0.1882 - val_accuracy: 0.9280 - val_mean_squared_error: 0.0536\n",
      "Epoch 3/50\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1797 - accuracy: 0.9291 - mean_squared_error: 0.0527 - val_loss: 0.1760 - val_accuracy: 0.9303 - val_mean_squared_error: 0.0502\n",
      "Epoch 4/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1710 - accuracy: 0.9331 - mean_squared_error: 0.0498 - val_loss: 0.1694 - val_accuracy: 0.9340 - val_mean_squared_error: 0.0488\n",
      "Epoch 5/50\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.1663 - accuracy: 0.9356 - mean_squared_error: 0.0483 - val_loss: 0.1697 - val_accuracy: 0.9355 - val_mean_squared_error: 0.0487\n",
      "Epoch 6/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1627 - accuracy: 0.9370 - mean_squared_error: 0.0473 - val_loss: 0.1706 - val_accuracy: 0.9371 - val_mean_squared_error: 0.0482\n",
      "Epoch 7/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1584 - accuracy: 0.9391 - mean_squared_error: 0.0456 - val_loss: 0.1601 - val_accuracy: 0.9391 - val_mean_squared_error: 0.0454\n",
      "Epoch 8/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1556 - accuracy: 0.9404 - mean_squared_error: 0.0450 - val_loss: 0.1597 - val_accuracy: 0.9395 - val_mean_squared_error: 0.0452\n",
      "Epoch 9/50\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1524 - accuracy: 0.9417 - mean_squared_error: 0.0440 - val_loss: 0.1540 - val_accuracy: 0.9429 - val_mean_squared_error: 0.0436\n",
      "Epoch 10/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1510 - accuracy: 0.9429 - mean_squared_error: 0.0434 - val_loss: 0.1538 - val_accuracy: 0.9416 - val_mean_squared_error: 0.0438\n",
      "Epoch 11/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1485 - accuracy: 0.9433 - mean_squared_error: 0.0427 - val_loss: 0.1554 - val_accuracy: 0.9423 - val_mean_squared_error: 0.0445\n",
      "Epoch 12/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1472 - accuracy: 0.9434 - mean_squared_error: 0.0423 - val_loss: 0.1554 - val_accuracy: 0.9432 - val_mean_squared_error: 0.0438\n",
      "Epoch 13/50\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.1468 - accuracy: 0.9437 - mean_squared_error: 0.0423 - val_loss: 0.1585 - val_accuracy: 0.9401 - val_mean_squared_error: 0.0451\n",
      "Epoch 14/50\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1452 - accuracy: 0.9448 - mean_squared_error: 0.0415 - val_loss: 0.1570 - val_accuracy: 0.9396 - val_mean_squared_error: 0.0454\n",
      "Epoch 15/50\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.1443 - accuracy: 0.9453 - mean_squared_error: 0.0412 - val_loss: 0.1464 - val_accuracy: 0.9452 - val_mean_squared_error: 0.0416\n",
      "Epoch 16/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1427 - accuracy: 0.9469 - mean_squared_error: 0.0407 - val_loss: 0.1467 - val_accuracy: 0.9437 - val_mean_squared_error: 0.0415\n",
      "Epoch 17/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1419 - accuracy: 0.9465 - mean_squared_error: 0.0406 - val_loss: 0.1472 - val_accuracy: 0.9433 - val_mean_squared_error: 0.0416\n",
      "Epoch 18/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1413 - accuracy: 0.9469 - mean_squared_error: 0.0404 - val_loss: 0.1464 - val_accuracy: 0.9475 - val_mean_squared_error: 0.0410\n",
      "Epoch 19/50\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.1394 - accuracy: 0.9467 - mean_squared_error: 0.0399 - val_loss: 0.1441 - val_accuracy: 0.9445 - val_mean_squared_error: 0.0408\n",
      "Epoch 20/50\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.1384 - accuracy: 0.9482 - mean_squared_error: 0.0395 - val_loss: 0.1427 - val_accuracy: 0.9465 - val_mean_squared_error: 0.0403\n",
      "Epoch 21/50\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.1370 - accuracy: 0.9486 - mean_squared_error: 0.0392 - val_loss: 0.1401 - val_accuracy: 0.9497 - val_mean_squared_error: 0.0393\n",
      "Epoch 22/50\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.1369 - accuracy: 0.9489 - mean_squared_error: 0.0392 - val_loss: 0.1413 - val_accuracy: 0.9459 - val_mean_squared_error: 0.0400\n",
      "Epoch 23/50\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.1352 - accuracy: 0.9488 - mean_squared_error: 0.0387 - val_loss: 0.1410 - val_accuracy: 0.9473 - val_mean_squared_error: 0.0392\n",
      "Epoch 24/50\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.1356 - accuracy: 0.9488 - mean_squared_error: 0.0389 - val_loss: 0.1468 - val_accuracy: 0.9464 - val_mean_squared_error: 0.0407\n",
      "Epoch 25/50\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1349 - accuracy: 0.9495 - mean_squared_error: 0.0384 - val_loss: 0.1375 - val_accuracy: 0.9480 - val_mean_squared_error: 0.0387\n",
      "Epoch 26/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1335 - accuracy: 0.9495 - mean_squared_error: 0.0382 - val_loss: 0.1456 - val_accuracy: 0.9455 - val_mean_squared_error: 0.0412\n",
      "Epoch 27/50\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1334 - accuracy: 0.9500 - mean_squared_error: 0.0381 - val_loss: 0.1371 - val_accuracy: 0.9493 - val_mean_squared_error: 0.0385\n",
      "Epoch 28/50\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1332 - accuracy: 0.9506 - mean_squared_error: 0.0377 - val_loss: 0.1549 - val_accuracy: 0.9416 - val_mean_squared_error: 0.0445\n",
      "Epoch 29/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1323 - accuracy: 0.9499 - mean_squared_error: 0.0378 - val_loss: 0.1471 - val_accuracy: 0.9433 - val_mean_squared_error: 0.0420\n",
      "Epoch 30/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1325 - accuracy: 0.9504 - mean_squared_error: 0.0377 - val_loss: 0.1476 - val_accuracy: 0.9432 - val_mean_squared_error: 0.0420\n",
      "Epoch 31/50\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.1316 - accuracy: 0.9500 - mean_squared_error: 0.0378 - val_loss: 0.1457 - val_accuracy: 0.9460 - val_mean_squared_error: 0.0409\n",
      "Epoch 32/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1314 - accuracy: 0.9503 - mean_squared_error: 0.0375 - val_loss: 0.1555 - val_accuracy: 0.9419 - val_mean_squared_error: 0.0441\n",
      "Epoch 33/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1302 - accuracy: 0.9526 - mean_squared_error: 0.0369 - val_loss: 0.1345 - val_accuracy: 0.9491 - val_mean_squared_error: 0.0376\n",
      "Epoch 34/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1303 - accuracy: 0.9510 - mean_squared_error: 0.0372 - val_loss: 0.1392 - val_accuracy: 0.9465 - val_mean_squared_error: 0.0395\n",
      "Epoch 35/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1291 - accuracy: 0.9515 - mean_squared_error: 0.0368 - val_loss: 0.1416 - val_accuracy: 0.9471 - val_mean_squared_error: 0.0400\n",
      "Epoch 36/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1290 - accuracy: 0.9515 - mean_squared_error: 0.0366 - val_loss: 0.1466 - val_accuracy: 0.9439 - val_mean_squared_error: 0.0421\n",
      "Epoch 37/50\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.1291 - accuracy: 0.9512 - mean_squared_error: 0.0370 - val_loss: 0.1394 - val_accuracy: 0.9467 - val_mean_squared_error: 0.0397\n",
      "Epoch 38/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1283 - accuracy: 0.9519 - mean_squared_error: 0.0366 - val_loss: 0.1329 - val_accuracy: 0.9493 - val_mean_squared_error: 0.0376\n",
      "Epoch 39/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1279 - accuracy: 0.9515 - mean_squared_error: 0.0367 - val_loss: 0.1347 - val_accuracy: 0.9485 - val_mean_squared_error: 0.0379\n",
      "Epoch 40/50\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.1274 - accuracy: 0.9522 - mean_squared_error: 0.0364 - val_loss: 0.1415 - val_accuracy: 0.9460 - val_mean_squared_error: 0.0400\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1274 - accuracy: 0.9519 - mean_squared_error: 0.0364 - val_loss: 0.1346 - val_accuracy: 0.9505 - val_mean_squared_error: 0.0378\n",
      "Epoch 42/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1267 - accuracy: 0.9516 - mean_squared_error: 0.0362 - val_loss: 0.1305 - val_accuracy: 0.9496 - val_mean_squared_error: 0.0372\n",
      "Epoch 43/50\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.1258 - accuracy: 0.9517 - mean_squared_error: 0.0361 - val_loss: 0.1401 - val_accuracy: 0.9465 - val_mean_squared_error: 0.0402\n",
      "Epoch 44/50\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.1262 - accuracy: 0.9522 - mean_squared_error: 0.0360 - val_loss: 0.1366 - val_accuracy: 0.9476 - val_mean_squared_error: 0.0386\n",
      "Epoch 45/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1254 - accuracy: 0.9532 - mean_squared_error: 0.0358 - val_loss: 0.1298 - val_accuracy: 0.9489 - val_mean_squared_error: 0.0372\n",
      "Epoch 46/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1256 - accuracy: 0.9527 - mean_squared_error: 0.0360 - val_loss: 0.1294 - val_accuracy: 0.9507 - val_mean_squared_error: 0.0368\n",
      "Epoch 47/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1252 - accuracy: 0.9527 - mean_squared_error: 0.0357 - val_loss: 0.1305 - val_accuracy: 0.9511 - val_mean_squared_error: 0.0369\n",
      "Epoch 48/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1245 - accuracy: 0.9528 - mean_squared_error: 0.0356 - val_loss: 0.1321 - val_accuracy: 0.9511 - val_mean_squared_error: 0.0376\n",
      "Epoch 49/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1240 - accuracy: 0.9527 - mean_squared_error: 0.0354 - val_loss: 0.1313 - val_accuracy: 0.9485 - val_mean_squared_error: 0.0377\n",
      "Epoch 50/50\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.1239 - accuracy: 0.9532 - mean_squared_error: 0.0355 - val_loss: 0.1332 - val_accuracy: 0.9516 - val_mean_squared_error: 0.0375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1b547f880d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.01\n",
    "bs = 32\n",
    "\n",
    "model = Sequential([\n",
    "        Dense(10, activation='tanh'),\n",
    "        Dense(10, activation='tanh'),\n",
    "        # output layer\n",
    "        Dense(2, activation='sigmoid'),\n",
    "    ])\n",
    "train_model(model, data, loss=\"binary_crossentropy\", momentum=0.9, learning_rate=lr, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will look now at how the results look. I will start by computing the PSxG of all the shots in the dataset provided initially. I used the scaled version of the data to make the predictions. To make the predictions was as simple as to use the model.predict(...) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(np.array(X))\n",
    "y_pred = pd.DataFrame(model.predict(X_scaled)[:,0],columns=['is_goal_pred'])\n",
    "y_pred = pd.merge(df_shots[['destination_pos_y','destination_pos_z']],y_pred,how = 'left',left_index = True, right_index = True).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a heatmap of the goal locations with higher chances of scoring. To do so, I first created a grid representing the goal, with a size of 24x8 feet. Then, I assigned each shot to a cell of the grid based on their destination location. For each cell, I calculated the mean of the PSxG of the shots assigned to it. It is important to remember that when looking at the goal from the front, the x-axis corresponds to the destination_pos_y and the y-axis to the destination_pos_z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>destination_pos_z_bin</th>\n",
       "      <th>(0.0, 0.297]</th>\n",
       "      <th>(0.297, 0.594]</th>\n",
       "      <th>(0.594, 0.892]</th>\n",
       "      <th>(0.892, 1.189]</th>\n",
       "      <th>(1.189, 1.486]</th>\n",
       "      <th>(1.486, 1.783]</th>\n",
       "      <th>(1.783, 2.08]</th>\n",
       "      <th>(2.08, 2.377]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>destination_pos_y_bin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0.446, 0.451]</th>\n",
       "      <td>0.033903</td>\n",
       "      <td>0.043593</td>\n",
       "      <td>0.034465</td>\n",
       "      <td>0.022375</td>\n",
       "      <td>0.038257</td>\n",
       "      <td>0.047261</td>\n",
       "      <td>0.009049</td>\n",
       "      <td>0.066352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.451, 0.455]</th>\n",
       "      <td>0.346053</td>\n",
       "      <td>0.313216</td>\n",
       "      <td>0.302043</td>\n",
       "      <td>0.295869</td>\n",
       "      <td>0.316676</td>\n",
       "      <td>0.271397</td>\n",
       "      <td>0.327434</td>\n",
       "      <td>0.448472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.455, 0.46]</th>\n",
       "      <td>0.553038</td>\n",
       "      <td>0.526039</td>\n",
       "      <td>0.480541</td>\n",
       "      <td>0.419975</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.517860</td>\n",
       "      <td>0.498650</td>\n",
       "      <td>0.535954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.46, 0.464]</th>\n",
       "      <td>0.490357</td>\n",
       "      <td>0.503986</td>\n",
       "      <td>0.465265</td>\n",
       "      <td>0.450193</td>\n",
       "      <td>0.416652</td>\n",
       "      <td>0.438970</td>\n",
       "      <td>0.486051</td>\n",
       "      <td>0.530861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.464, 0.469]</th>\n",
       "      <td>0.422931</td>\n",
       "      <td>0.415188</td>\n",
       "      <td>0.405837</td>\n",
       "      <td>0.388271</td>\n",
       "      <td>0.370951</td>\n",
       "      <td>0.353492</td>\n",
       "      <td>0.382702</td>\n",
       "      <td>0.396626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.469, 0.473]</th>\n",
       "      <td>0.345188</td>\n",
       "      <td>0.354583</td>\n",
       "      <td>0.360374</td>\n",
       "      <td>0.296958</td>\n",
       "      <td>0.293736</td>\n",
       "      <td>0.305650</td>\n",
       "      <td>0.317423</td>\n",
       "      <td>0.337941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.473, 0.478]</th>\n",
       "      <td>0.308421</td>\n",
       "      <td>0.273318</td>\n",
       "      <td>0.266749</td>\n",
       "      <td>0.240319</td>\n",
       "      <td>0.263725</td>\n",
       "      <td>0.189948</td>\n",
       "      <td>0.268789</td>\n",
       "      <td>0.329635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.478, 0.482]</th>\n",
       "      <td>0.340737</td>\n",
       "      <td>0.210662</td>\n",
       "      <td>0.239164</td>\n",
       "      <td>0.244536</td>\n",
       "      <td>0.172348</td>\n",
       "      <td>0.172114</td>\n",
       "      <td>0.184460</td>\n",
       "      <td>0.273940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.482, 0.487]</th>\n",
       "      <td>0.272963</td>\n",
       "      <td>0.227986</td>\n",
       "      <td>0.190299</td>\n",
       "      <td>0.141305</td>\n",
       "      <td>0.165328</td>\n",
       "      <td>0.184549</td>\n",
       "      <td>0.169460</td>\n",
       "      <td>0.285440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.487, 0.491]</th>\n",
       "      <td>0.260289</td>\n",
       "      <td>0.193086</td>\n",
       "      <td>0.161004</td>\n",
       "      <td>0.120957</td>\n",
       "      <td>0.141377</td>\n",
       "      <td>0.130113</td>\n",
       "      <td>0.172427</td>\n",
       "      <td>0.158317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.491, 0.496]</th>\n",
       "      <td>0.266261</td>\n",
       "      <td>0.226177</td>\n",
       "      <td>0.159910</td>\n",
       "      <td>0.152987</td>\n",
       "      <td>0.126954</td>\n",
       "      <td>0.130951</td>\n",
       "      <td>0.164328</td>\n",
       "      <td>0.212499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.496, 0.5]</th>\n",
       "      <td>0.256475</td>\n",
       "      <td>0.176098</td>\n",
       "      <td>0.169491</td>\n",
       "      <td>0.151119</td>\n",
       "      <td>0.084741</td>\n",
       "      <td>0.131743</td>\n",
       "      <td>0.143097</td>\n",
       "      <td>0.160399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.5, 0.504]</th>\n",
       "      <td>0.272556</td>\n",
       "      <td>0.179654</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>0.170333</td>\n",
       "      <td>0.178371</td>\n",
       "      <td>0.160915</td>\n",
       "      <td>0.109366</td>\n",
       "      <td>0.180773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.504, 0.509]</th>\n",
       "      <td>0.239542</td>\n",
       "      <td>0.192819</td>\n",
       "      <td>0.183053</td>\n",
       "      <td>0.142238</td>\n",
       "      <td>0.142837</td>\n",
       "      <td>0.108845</td>\n",
       "      <td>0.140549</td>\n",
       "      <td>0.199102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.509, 0.513]</th>\n",
       "      <td>0.233231</td>\n",
       "      <td>0.207193</td>\n",
       "      <td>0.216521</td>\n",
       "      <td>0.167395</td>\n",
       "      <td>0.216223</td>\n",
       "      <td>0.161458</td>\n",
       "      <td>0.168375</td>\n",
       "      <td>0.213488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.513, 0.518]</th>\n",
       "      <td>0.246054</td>\n",
       "      <td>0.252411</td>\n",
       "      <td>0.225226</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>0.142474</td>\n",
       "      <td>0.271758</td>\n",
       "      <td>0.240218</td>\n",
       "      <td>0.226901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.518, 0.522]</th>\n",
       "      <td>0.245317</td>\n",
       "      <td>0.242257</td>\n",
       "      <td>0.210939</td>\n",
       "      <td>0.203229</td>\n",
       "      <td>0.133176</td>\n",
       "      <td>0.189179</td>\n",
       "      <td>0.175577</td>\n",
       "      <td>0.248546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.522, 0.527]</th>\n",
       "      <td>0.312837</td>\n",
       "      <td>0.296169</td>\n",
       "      <td>0.250376</td>\n",
       "      <td>0.223441</td>\n",
       "      <td>0.205519</td>\n",
       "      <td>0.278929</td>\n",
       "      <td>0.258113</td>\n",
       "      <td>0.316070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.527, 0.531]</th>\n",
       "      <td>0.355638</td>\n",
       "      <td>0.312077</td>\n",
       "      <td>0.324788</td>\n",
       "      <td>0.285454</td>\n",
       "      <td>0.252391</td>\n",
       "      <td>0.329787</td>\n",
       "      <td>0.307652</td>\n",
       "      <td>0.321776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.531, 0.536]</th>\n",
       "      <td>0.432269</td>\n",
       "      <td>0.424102</td>\n",
       "      <td>0.367751</td>\n",
       "      <td>0.368545</td>\n",
       "      <td>0.366750</td>\n",
       "      <td>0.337314</td>\n",
       "      <td>0.355916</td>\n",
       "      <td>0.464374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.536, 0.54]</th>\n",
       "      <td>0.478764</td>\n",
       "      <td>0.514951</td>\n",
       "      <td>0.446162</td>\n",
       "      <td>0.462868</td>\n",
       "      <td>0.393982</td>\n",
       "      <td>0.443290</td>\n",
       "      <td>0.531999</td>\n",
       "      <td>0.511795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.54, 0.545]</th>\n",
       "      <td>0.525368</td>\n",
       "      <td>0.575373</td>\n",
       "      <td>0.526463</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.474195</td>\n",
       "      <td>0.554904</td>\n",
       "      <td>0.583798</td>\n",
       "      <td>0.566927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.545, 0.549]</th>\n",
       "      <td>0.443224</td>\n",
       "      <td>0.438990</td>\n",
       "      <td>0.343883</td>\n",
       "      <td>0.342287</td>\n",
       "      <td>0.350574</td>\n",
       "      <td>0.363665</td>\n",
       "      <td>0.494369</td>\n",
       "      <td>0.550752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.549, 0.554]</th>\n",
       "      <td>0.073279</td>\n",
       "      <td>0.091063</td>\n",
       "      <td>0.076852</td>\n",
       "      <td>0.055598</td>\n",
       "      <td>0.066769</td>\n",
       "      <td>0.042709</td>\n",
       "      <td>0.028239</td>\n",
       "      <td>0.109320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "destination_pos_z_bin  (0.0, 0.297]  (0.297, 0.594]  (0.594, 0.892]  \\\n",
       "destination_pos_y_bin                                                 \n",
       "(0.446, 0.451]             0.033903        0.043593        0.034465   \n",
       "(0.451, 0.455]             0.346053        0.313216        0.302043   \n",
       "(0.455, 0.46]              0.553038        0.526039        0.480541   \n",
       "(0.46, 0.464]              0.490357        0.503986        0.465265   \n",
       "(0.464, 0.469]             0.422931        0.415188        0.405837   \n",
       "(0.469, 0.473]             0.345188        0.354583        0.360374   \n",
       "(0.473, 0.478]             0.308421        0.273318        0.266749   \n",
       "(0.478, 0.482]             0.340737        0.210662        0.239164   \n",
       "(0.482, 0.487]             0.272963        0.227986        0.190299   \n",
       "(0.487, 0.491]             0.260289        0.193086        0.161004   \n",
       "(0.491, 0.496]             0.266261        0.226177        0.159910   \n",
       "(0.496, 0.5]               0.256475        0.176098        0.169491   \n",
       "(0.5, 0.504]               0.272556        0.179654        0.147920   \n",
       "(0.504, 0.509]             0.239542        0.192819        0.183053   \n",
       "(0.509, 0.513]             0.233231        0.207193        0.216521   \n",
       "(0.513, 0.518]             0.246054        0.252411        0.225226   \n",
       "(0.518, 0.522]             0.245317        0.242257        0.210939   \n",
       "(0.522, 0.527]             0.312837        0.296169        0.250376   \n",
       "(0.527, 0.531]             0.355638        0.312077        0.324788   \n",
       "(0.531, 0.536]             0.432269        0.424102        0.367751   \n",
       "(0.536, 0.54]              0.478764        0.514951        0.446162   \n",
       "(0.54, 0.545]              0.525368        0.575373        0.526463   \n",
       "(0.545, 0.549]             0.443224        0.438990        0.343883   \n",
       "(0.549, 0.554]             0.073279        0.091063        0.076852   \n",
       "\n",
       "destination_pos_z_bin  (0.892, 1.189]  (1.189, 1.486]  (1.486, 1.783]  \\\n",
       "destination_pos_y_bin                                                   \n",
       "(0.446, 0.451]               0.022375        0.038257        0.047261   \n",
       "(0.451, 0.455]               0.295869        0.316676        0.271397   \n",
       "(0.455, 0.46]                0.419975        0.380435        0.517860   \n",
       "(0.46, 0.464]                0.450193        0.416652        0.438970   \n",
       "(0.464, 0.469]               0.388271        0.370951        0.353492   \n",
       "(0.469, 0.473]               0.296958        0.293736        0.305650   \n",
       "(0.473, 0.478]               0.240319        0.263725        0.189948   \n",
       "(0.478, 0.482]               0.244536        0.172348        0.172114   \n",
       "(0.482, 0.487]               0.141305        0.165328        0.184549   \n",
       "(0.487, 0.491]               0.120957        0.141377        0.130113   \n",
       "(0.491, 0.496]               0.152987        0.126954        0.130951   \n",
       "(0.496, 0.5]                 0.151119        0.084741        0.131743   \n",
       "(0.5, 0.504]                 0.170333        0.178371        0.160915   \n",
       "(0.504, 0.509]               0.142238        0.142837        0.108845   \n",
       "(0.509, 0.513]               0.167395        0.216223        0.161458   \n",
       "(0.513, 0.518]               0.159800        0.142474        0.271758   \n",
       "(0.518, 0.522]               0.203229        0.133176        0.189179   \n",
       "(0.522, 0.527]               0.223441        0.205519        0.278929   \n",
       "(0.527, 0.531]               0.285454        0.252391        0.329787   \n",
       "(0.531, 0.536]               0.368545        0.366750        0.337314   \n",
       "(0.536, 0.54]                0.462868        0.393982        0.443290   \n",
       "(0.54, 0.545]                0.485149        0.474195        0.554904   \n",
       "(0.545, 0.549]               0.342287        0.350574        0.363665   \n",
       "(0.549, 0.554]               0.055598        0.066769        0.042709   \n",
       "\n",
       "destination_pos_z_bin  (1.783, 2.08]  (2.08, 2.377]  \n",
       "destination_pos_y_bin                                \n",
       "(0.446, 0.451]              0.009049       0.066352  \n",
       "(0.451, 0.455]              0.327434       0.448472  \n",
       "(0.455, 0.46]               0.498650       0.535954  \n",
       "(0.46, 0.464]               0.486051       0.530861  \n",
       "(0.464, 0.469]              0.382702       0.396626  \n",
       "(0.469, 0.473]              0.317423       0.337941  \n",
       "(0.473, 0.478]              0.268789       0.329635  \n",
       "(0.478, 0.482]              0.184460       0.273940  \n",
       "(0.482, 0.487]              0.169460       0.285440  \n",
       "(0.487, 0.491]              0.172427       0.158317  \n",
       "(0.491, 0.496]              0.164328       0.212499  \n",
       "(0.496, 0.5]                0.143097       0.160399  \n",
       "(0.5, 0.504]                0.109366       0.180773  \n",
       "(0.504, 0.509]              0.140549       0.199102  \n",
       "(0.509, 0.513]              0.168375       0.213488  \n",
       "(0.513, 0.518]              0.240218       0.226901  \n",
       "(0.518, 0.522]              0.175577       0.248546  \n",
       "(0.522, 0.527]              0.258113       0.316070  \n",
       "(0.527, 0.531]              0.307652       0.321776  \n",
       "(0.531, 0.536]              0.355916       0.464374  \n",
       "(0.536, 0.54]               0.531999       0.511795  \n",
       "(0.54, 0.545]               0.583798       0.566927  \n",
       "(0.545, 0.549]              0.494369       0.550752  \n",
       "(0.549, 0.554]              0.028239       0.109320  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_bins = [y_pred[\"destination_pos_y\"].min()+(y_pred[\"destination_pos_y\"].max() - y_pred[\"destination_pos_y\"].min())/24*x for x in range(0,25)]\n",
    "z_bins = [y_pred[\"destination_pos_z\"].min()+(y_pred[\"destination_pos_z\"].max() - y_pred[\"destination_pos_z\"].min())/8*x for x in range(0,9)]\n",
    "\n",
    "y_pred['destination_pos_y_bin'] = pd.cut(y_pred[\"destination_pos_y\"], bins=y_bins)\n",
    "y_pred['destination_pos_z_bin'] = pd.cut(y_pred[\"destination_pos_z\"], bins=z_bins)\n",
    "y_pred = y_pred.drop([\"destination_pos_y\",'destination_pos_z'],axis=1)\n",
    "y_pred = y_pred.groupby(['destination_pos_y_bin','destination_pos_z_bin']).mean()\n",
    "hist = y_pred.pivot_table(values='is_goal_pred', index=\"destination_pos_y_bin\", columns='destination_pos_z_bin')\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1b5494be220>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5), dpi=100)\n",
    "im = ax.imshow(hist.T, cmap=\"Blues\",origin='lower', aspect='auto')\n",
    "ax.set_title(\"Goal heatmap\")\n",
    "fig.colorbar(im, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
